 **Hierarchical Similarity Detection for Post-Deduplication Delta Compression**
#### Idea
1. N-transformæˆ–[[Odess]]ä¼šå°†12ä¸ªoriç‰¹å¾çº¿æ€§å˜æ¢ä¸º3ä¸ªSFï¼Œæ¯ä¸ªSF hashè¡¨ç”±4ä¸ªåŸæœ¬oriç”Ÿæˆè€Œæ¥ æ‰€ä»¥å‘½ä¸­çš„ç›¸ä¼¼åº¦è¦æ±‚æ¯”è¾ƒé«˜ã€‚æœ¬æ–‡ç”¨è®¾ç½®äº†ä¸‰å±‚SF hashè¡¨ã€‚3\*4 4\*3 6\*2ï¼Œè‡ªä¸Šè€Œä¸‹ç›¸ä¼¼åº¦è¦æ±‚è¶Šæ¥è¶Šä½ï¼Œä½†æ˜¯åé¢çš„å±‚èƒ½æ‰¾åˆ°åŸæœ¬æ‰¾ä¸åˆ°çš„å°ç›¸ä¼¼å—ï¼Œå‡å°‘å‡é˜´æ€§ã€‚//å•ä¸€çš„é˜ˆå€¼ä¸èƒ½åŒæ—¶å‡å°‘å‡é˜³æ€§å’Œå‡é˜´æ€§çš„æ•°é‡ï¼Œå¦‚æœå•ä¸€çš„é˜ˆå€¼ç›´æ¥ç”¨6\*2çš„è¯ï¼Œåˆ™æœ¬è¯¥èƒ½æ‰¾åˆ°ç›¸ä¼¼åº¦æ›´é«˜çš„å—æ—¶ï¼Œä¼šå‘½ä¸­ä¸€äº›ç›¸ä¼¼åº¦ä½çš„å—ã€‚
2. æœ¬æ–‡å°†SF hashå€¼ç§°ä½œå—çš„metadataï¼Œç”±äºæ¯”ä¼ ç»Ÿæ–¹æ³•å¤šä¸¤å±‚10ï¼ˆ4+6ï¼‰ä¸ªè¡¨ï¼Œæ‰€ä»¥éœ€è¦å‡å°‘ä»–çš„ç©ºé—´å¼€é”€ï¼Œäºæ˜¯æå‡ºa metadata lifecycle managerä¼šç»´æŠ¤ä¸‹ä¸¤å±‚çš„SFçš„ç”Ÿå‘½å‘¨æœŸï¼Œå®šæ—¶åˆ é™¤æ—§SFã€‚è¿™ä¹Ÿä¼šå¯¼è‡´ä¸€äº›é—®é¢˜ï¼Œä»¥è‡³äºé‡å¤çš„å—ä¹Ÿè¦å»ç®—SFæ¥æŸ¥çœ‹è‡ªå·±åŸæœ¬çš„SFä¹Ÿæ²¡æœ‰è¢«åˆ é™¤ã€‚
3. æœ€åå¼•å…¥äº†ä¸€ä¸ªå‡é˜³æ€§è¿‡æ»¤å™¨æ¥ä¸¢å¼ƒå¯¹æ•´ä½“æ•°æ®å‡å°‘ä¸åˆ©çš„åŒ¹é…å—ï¼Œè¿‡æ»¤ç”¨çš„æŒ‡æ ‡æ˜¯è‡ªæ­¤ä¹‹å‰çš„Compression rabioã€‚
- åœ¨è¿™é‡Œçš„å‡é˜³æ€§çš„å®šä¹‰æ˜¯ä¸æ˜¯å¤ªè‹›åˆ»äº†ï¼ŸåŸæ–‡ï¼šflags the base block as a false positive if its compressibility is below the lossless compression ratio averaged over a sliding window of previous chunks.
#### Readme
æå‡ºä¼ ç»Ÿæ–¹æ³•åªæœ‰ä¸€æ®µSFé˜ˆå€¼æ—¶ï¼Œå­˜åœ¨åœ¨ç²¾åº¦ä¸å‡é˜´æ€§ä¹‹é—´è¿›é€€ä¸¤éš¾çš„é—®é¢˜ï¼Œç„¶åè‡ªç„¶åœ°æå‡ºäº†3å±‚SF hashè¡¨çš„è®¾è®¡ä½œä¸ºæ ¸å¿ƒå·¥ä½œã€‚
~~ä¸ºäº†è§£å†³æ–°å¢çš„åä¸¤å±‚SF hashè¡¨çš„å¸¦æ¥ç©ºé—´å¼€é”€ï¼Œä»–å®šæœŸåˆ é™¤åä¸¤å±‚çš„hashï¼Œè¿™æ ·åšæ˜¯å¯ä»¥åšï¼Œè€Œä¸”ä¹Ÿç¡®å®åœ¨æ¯ç§æ–‡ä»¶çš„ç¬¬å…«ä¸ªversionä¹‹åå¯ä»¥æŠŠå“ˆå¸Œè¡¨å¼€é”€çš„å¢é•¿è¿˜åŸå›äº†N-transformçš„å¢é•¿ç¨‹åº¦ã€‚ä½†æ˜¯ï¼Œå®ç°èµ·æ¥æœ‰é—®é¢˜ã€‚é¦–å…ˆï¼Œä¸å¯èƒ½åœ¨chunkinfoé‡Œå­˜åä¸¤å±‚SF Hashå§ï¼Œä¸ç„¶æœ¬æœ«å€’ç½®äº†ï¼Œä½ å¼€å§‹å°±åˆ«åœ¨chunkinfoé‡Œå­˜ä¸æ˜¯æ›´å¥½å—ï¼Ÿç›´æ¥å°‘å­˜ä¸€å€ã€‚é‚£å¯è¡Œçš„åŠæ³•å°±æ˜¯å»æ‰¾å…«ä¸ªç‰ˆæœ¬å‰çš„4+6ä¸ªSFï¼Œrestoreå›æ¥ç„¶åå»é‡ç®—ä¸¤è½®SFï¼Œæ—¶é—´å¼€é”€å¤ªå¤§äº†ã€‚è¿˜æ˜¯è¯´valueä¸åªæ˜¯å—å·ï¼Œå†åŠ ä¸€ä¸ªç‰ˆæœ¬å·ï¼Œéå†ä¸€éç¬¬äºŒå±‚4ä¸ªè¡¨å’Œç¬¬ä¸‰å±‚6ä¸ªè¡¨ã€‚~~//æˆ‘çš„é—®é¢˜
å‡é˜³æ€§çš„åˆ¤æ–­é—®é¢˜


#### Limitations of traditional methodsï¼š
the first-fit approach of current super-feature-based systems does not allow them to quantify and discriminate the degree of similarity between similar base block candidates for a new block, leaving them stuck in this dilemma.
>ç›®å‰åŸºäºè¶…çº§ç‰¹å¾çš„ç³»ç»Ÿçš„ç¬¬ä¸€æ‹Ÿåˆæ–¹æ³•ä¸å…è®¸ä»–ä»¬é‡åŒ–å’ŒåŒºåˆ†ä¸€ä¸ªæ–°å—çš„ç›¸ä¼¼çš„å€™é€‰åŸºå—ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œè¿™ä½¿ä»–ä»¬**è¿›é€€ä¸¤éš¾**ï¼ˆç²¾åº¦å’Œå‡é˜´æ€§ï¼‰ã€‚
>ç„¶åæœ¬æ–‡æå‡ºç”¨ä¸‰å±‚ç›¸ä¼¼åº¦é˜ˆå€¼ä¸åŒçš„SF Hashè¡¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
>//å¾ˆè‡ªç„¶çš„è§£å†³æ–¹æ¡ˆï¼Œä½†æ˜¯æˆ‘è§‰å¾—åº”è¯¥å¯ä»¥ç”¨oriç‰¹å¾å»é‡åŒ–ç›¸ä¼¼åº¦ï¼Œåªæ˜¯ä¸è®¾ç½®é˜ˆå€¼å¾ˆéš¾ç¡®å®šä»€ä¹ˆæ—¶å€™ç»“æŸæœç´¢ï¼Œæ—¶é—´å¼€é”€å¤ªå¤§äº†ï¼Œå¯èƒ½è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆNTransformè¦è®¾ç½®SFå§ï¼Ÿé€ä¸ªæ¯”oriå¤ªæŠ½è±¡äº†ï¼Œè¿˜ä¸å¦‚è¿™æ ·è®¾ä¸‰å±‚é˜ˆå€¼ï¼Œä½†æ˜¯ç”¨oriç‰¹å¾æ›´çµæ´»ï¼Ÿè¿™é‡Œå†æƒ³æƒ³ã€‚
#### The work of this articleï¼š
1. Palantir introduces hierarchical super features with different sensitivities to block similarities to find more candidates of similar blocks
![[Pasted image 20240531104902.png]]
2. The overhead of Palantir for storing additional super-features is overcome by exploiting temporal localities of backup streams
>The default setting of Palantir keeps Tier-1 SFs for all versions, Tier-2 SFs for 5 versions, and Tier-3 SFs for 2 versions. Compared to existing solutions with only one SF tier, the additional metadata of Palantir comes from the Tier-2 and Tier-3 SFs of the last five and two versions, respectively. This metadata overhead is a constant and does not increase as more backup versions are stored.
![[Pasted image 20240531105925.png]]
3. Palantir also introduces a false positive filter to discard matching blocks that are counterproductive for the overall data reduction.
>The key idea of Palantirâ€™s false positive filter is to replace the exact comparison with an estimate by comparing the delta compression ratio of the current block (including the lossless compression of the delta encoding library) with that of the previous ğ¿ lossless compression records. If this delta compression ratio is higher than the average value of those ğ¿ records, we mark it as a true positive; otherwise as a false

![[Pasted image 20240531110337.png]]



```c++
void Palantir::CleanIndex()

{

Â  Â  cout << "clear and the version is " << Version - 5 << " and " << Version - 2 << endl;

  

Â  Â  // æ¸…ç† SFindex2

Â  Â  for (auto it = SFindex2.begin(); it != SFindex2.end();)

Â  Â  {

Â  Â  Â  Â  if (!it->second.empty() && it->second[0].version < Version - 5)

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  it->second.erase(it->second.begin());

Â  Â  Â  Â  Â  Â  SFDelete++;

Â  Â  Â  Â  }

  

Â  Â  Â  Â  // å¦‚æœ key å¯¹åº”çš„ vector ä¸ºç©ºï¼Œåˆ™åˆ é™¤æ•´ä¸ª key

Â  Â  Â  Â  if (it->second.empty())

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  it = SFindex2.erase(it);

Â  Â  Â  Â  }

Â  Â  Â  Â  else

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  ++it;

Â  Â  Â  Â  }

Â  Â  }

  

Â  Â  // æ¸…ç† SFindex3

Â  Â  for (auto it = SFindex3.begin(); it != SFindex3.end();)

Â  Â  {

Â  Â  Â  Â  if (!it->second.empty() && it->second[0].version < Version - 2)

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  it->second.erase(it->second.begin());

Â  Â  Â  Â  Â  Â  SFDelete++;

Â  Â  Â  Â  }

  

Â  Â  Â  Â  // å¦‚æœ key å¯¹åº”çš„ vector ä¸ºç©ºï¼Œåˆ™åˆ é™¤æ•´ä¸ª key

Â  Â  Â  Â  if (it->second.empty())

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  it = SFindex3.erase(it);

Â  Â  Â  Â  }

Â  Â  Â  Â  else

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  ++it;

Â  Â  Â  Â  }

Â  Â  }

}
```
