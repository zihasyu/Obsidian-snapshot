  

### 收集数据阶段

如果只跑一个snapshot，重复的模式可能太少，假设按两个算，即4GiB.

按固定长来切则有2^19个块，需要存储每个块与另外所有块的delta compression ratio(8B)，即2^38个数字，合计2TiB大小的delta compression ratio数据需要存储。

哪怕是2GiB的数据集也有500GiB的ratio数据，不可能全放在内存里，只能放进外存且不停地io，这会使得速度过慢。

(BiSearch做BruteForce的空间开销是O(n)的，但做聚类需要O(n^2))

### 聚类阶段

向张嘉师兄咨询过这点，一般的聚类方案还是需要有非距离的属性来描述其位置，因为中心点迭代的时候需要存放在一个坐标（这个坐标不是实际存在的chunk）上（如果拿chunkid作为坐标轴的话没有意义，chunkid不能表示相似性或者chunk的特征，这会使得其）。

当然师兄说也存在只需要距离来迭代的算法，例如K-Medoids，他是试探性地会拿不同的点作为中心点去在此计算整体的距离是否有收益，但只靠距离的聚类时间开销会比其余聚类更大。

且该算法本身的limitation也是不能应用到较大的数据集。

### 这样做的意义

做delta compression的开销很大，光合基金给的算法范例流程图也是直接 算feature→分组→local compression。该基金要求的吞吐量计算是按照分组之前的时间算的，我觉得不能接受在分组之前做任何compression的试探，用delta compression的结果来指导分组显然是不满足分组吞吐量要求的。

所以这个做法**不能写进预期研究方案**里。

//我们的baseline方案只用算一遍特征值就完成了分组，这个方案在非二进制数据集上的表现还可以。感觉**预期研究方案**还是要围绕特征值去做的。

如果仅仅是作为类似于optimal的方案用于我们后续的课题参考做结果的对比，我觉得可以用更小力度的数据集去跑，比方说2个snapshot，每个snapshot取前2^10个块等等。同时我们的方案也只跑前2^10个块，这样来对比结果。

此外，第二阶段的聚类目标是使距离和(ratio倒数，或者savesize和)最小，如果想通过剪支或者别的优化来在一个**可行的时间**内完成求次优解，这个任务有些像ILP那个课题。