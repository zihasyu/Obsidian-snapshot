# Introduction

1. 阐述我们针对的场景，即集成了数据去重和增量压缩的备份文件存储系统。
    
2. 传统的方法进行数据去冗余的方式是数据去重+feature-based的增量压缩，存在一些问题：
    
    1. feature-based方法只能找到数据内容较为相似的相似块，对于一些相似程度不高的块会有遗漏，存在一个accuracy-versus-coverage dilemma。设计实验论证。CDF图。现有方法，feature-based + bf data reduction breakdown ted
        
    2. Fastcdc + tar 俩阶段chunking
        
    3. ~~现有的基于内容的切块方式存在boundary shifting的问题，对于数据去重和增量压缩都存在一定程度的影响，并且只考虑了基于内容的chunking并没有考虑到备份文件本身可能存在的一些元数据。~~
        
    4. feature-based方法为了解决accuracy-versus-coverage dilemma的方法会引入很多index开销和feature计算的开销(Palantir)。base indentification storage efficency index computation 三个维度超过现有方法
        
3. 介绍我们的contribution：
    
    1. BiSearch的设计利用backup locality来解决accuracy-versus-coverage dilemma，找到更多的delta chunk。
        
    2. 基于metadata的chunking方式，来解决boundary shifting的问题，能显著提高数据去冗余的效果。
        
    3. 利用locality可以显著减少index开销。
        
4. 简单说明下实验结果，在每个数据集上的提升。
    

# Background

1. Background on data reduction for backup storage
    
    1. Deduplication
        
    2. Delta compression
        
2. 介绍Feature-based的相关工作。
    
    1. Finesse
        
    2. Odess
        
    3. Palantir
        
    4. NTransform-SF
        
3. 介绍CDC切块的相关工作。
    
    1. Fastcdc
        
    2. Gear
        
    3. Mtar
        

# Motivation & Challenges

## Motivation

1. feature-based方法只能找到数据内容较为相似的相似块，对于一些相似程度不高的块会有遗漏，存在一个accuracy-versus-coverage dilemma。但如果类似于palantir去降低feature-based方法的准确性约束会引入极大的index开销。而在备份workload中本身存在比较好的locality，利用locality来寻找可能的相似块可以address到这一点。这里可以有个实验来说明这点。
    

暂时无法在成电飞书文档外展示此内容

2. 基于元数据的chunking方式可以更好的适配delta compression。现有的数据去冗余的方式是data chunking + deduplication + post deduplication delta compression。但现有的chunking方式会因为较小的数据改动而导致严重的boundary shifting，使data reduction的performance下降。在接近理想的chunking方式下其实数据去冗余的工作流可以变成data chunking + delta compression，（这里感觉可以以git的delta compression机制作为一个例子来说明这个motivation的合理性）。
    

暂时无法在成电飞书文档外展示此内容

## Challenges

1. 直接使用locality不可取，locality选择的很多basechunk效果很差，多做了很多delta compression但是效果很差。需要设计一个合理的方式进行对locality的定位方式进行优化。（BiSearch）
    
2. 如何利用元数据进行chunking对于backup中存在较大文件时还是不能很好的解决边界偏移的问题（TODO1 如何优化大文件的chunking；TODO2 VMDK类型是否可以实现基于元数据chunking，目前调研下来VMDK也存在一些header有元数据，但是可能要配合文件系统的一些操作才能实现）
    

# Design

## Metadata Based Chunking 语义chunking

1. Tar包中元数据和数据进行了很规整的划分，可以利用其中的元数据确定chunking的切点，从而降低边界偏移出现的概率。
    
2. 对于大文件，可以在指定的边界内使用fastcdc等基于内容的chunking方式（待定）
    
3. vmdk文件上设计一种使用元数据知道data chunking的方式（待定）
    

## BiSearch

1. 设定阈值来过滤一些压缩效果不好的basechunk。
    
2. 对于效果不好的chunk，使用odess或其他feature-based方式来纠正定位。
    
3. 设置自动化阈值（待定，目前效果比较好的metric是logical chunk的lz4平均压缩率）
    

## Metadata Management

# Evaluation

## Overall Compression Ratio

|   |   |   |   |
|---|---|---|---|
|Overall ratio|cassandra|linux|web|
|BiSearch+Tar|9.26 (-46.7%)|21.77(+47.4%)|191.22(+107.4%)|
|Odess+FastCDC|17.384|14.769|92.191|

## Delta Compression Ratio

## Compression Ratio contribution Breakdown

## LZ4_Ratio Impact

  

![](https://uestc.feishu.cn/space/api/box/stream/download/asynccode/?code=ODAwYmM4YWE2OTQ5MmE0Y2I0ZmFlZGQ5YTJlNjliNjhfbDdSRzloZnlRWWRyaVVRaWdPZmlHMjg3dkFxYUpwanVfVG9rZW46STdEbmI3Qnpxb2lya1J4R1VLV2M4bE04bnpkXzE3NDE0Mzc5NzU6MTc0MTQ0MTU3NV9WNA)