

```latex
To address the boundary instability problem observed in our motivation study, we must ensure the preservation of file boundaries to prevent boundary shift propagation between files.
Within the tar archive format, file boundaries can be precisely captured by parsing headers and their encapsulated metadata. According to the UNIX POSIX Tar specification, tar archives consist of 512B headers, variable-length files, and additional 512B metadata blocks. Each file is preceded by a header block that manages its metadata. By parsing the size field in these headers (as shown in Figure 3), we can determine exact file boundaries. For example, using the header's beginning as a reference point, a file's boundary would be defined as (Header+512, Header+512+size).

Additionally, we must determine the type of data to be processed next, as the size field is populated only when the subsequent data blocks constitute a file. When the following content represents special file entries containing only metadata (directories, symbolic links, and device files), the size value is zero. Therefore, relying solely on the size field for boundary determination is insufficient. We combine analysis of the type field to correctly identify the nature of subsequent blocks, enabling proper utilization of the size field.

Having established file boundaries (as illustrated in Figure 3), we obtain numerous chunks of various types (File, header, others). However, directly using these boundaries as final chunk delineations presents two significant challenges: First, headers and other metadata chunks at only 512B are excessively small, substantially increasing the metadata overhead for the backup system. Second, excessively large files treated as single chunks may compromise system performance and delta compression efficiency.

To address these challenges, we organize the data blocks into three distinct chunk types while preserving file boundaries, based on file size and type:

\begin{itemize}
    \item \textbf{File-chunks}: For files smaller than 4 MiB, we aggregate their data blocks into a single File-chunk. This strategy ensures efficient deduplication of unchanged files and captures modifications into one chunk, optimizing metadata-guided delta compression.
    
    \item \textbf{Multi-Meta-chunks}: For special file entries (directories, symbolic links, and device files) containing only metadata, we generate Multi-Meta-chunks from their headers, preserving essential information while minimizing storage overhead.
    
    \item \textbf{CDC-chunks} (Content-Defined chunks): For files larger than 4 MiB, we implement FastCDC to generate CDC-chunks. This approach maintains uniform chunk sizes within a specified range, enhancing delta compression efficiency by preventing chunk size misalignment.
\end{itemize}
```

```latex
为解决了我们在动机研究中观察到的边界不稳定问题，我们需要确保文件边界得到保留，以防止文件之间边界偏移的传播。
而在tar存档格式中，文件边界是可以通过解析header及其存放的元数据来捕获得到的。由 UNIX POSIX Tar 规范我们可以得知，tar存档由512B的headers,不定长的Files,及其他512B的元数据blocks组成，每个File一个会前置一个Header block来管理它的元数据，我们通过解析Header的size字段（如图3），便得到了每个File的文件边界(以将Header的头部当作起始点为例，那么该File的文件边界为(Header+512,Header+512+size))。

此外，我们必须确定接下来要处理的数据类型。这是因为Tar存档并不是一个完全只由Header与File相见交织而成的结构，它还有一部分other metadata blocks。如果按照普通Header的size解析方式去解析other metadata blocks，会得到错误的size以至于chunking越界。所以我们需要通过解析Header中的type字段来接下来要处理的数据类型。

至此，我们确定好了文件边界（如图3），得到大量各个类型的chunk(File,header,others)。但如果直接使用这些边界作为最终chunk边界存在两方面问题：其一，header和others 作为chunk只有512B太小了，这会极大增加备份系统的元数据开销。其二，可能存在过大的File，单独作为一个chunk也不利于{这里想想}。

为解决上述问题，对于上述数据blocks，我们在不破坏File边界的前提下根据其文件大小和类型整理三种类型的chunks，

\paragraph{多元块}：对于file header block和metadata header block，它们的大小统一只有512B，如果每个header单独作为chunk处理，会带来极大的元数据开销。所以我们的见解是将若干个(默认16个，因为备份系统中常用8KiB作为一个chunk的平均大小)header为一组，聚合为一个Multi-metadata chunk进行去冗余处理(无损压缩或增量压缩)与元数据管理。

传统场景中，将许多小chunk合并为一个chunk一般是从存储率到元数据开销的一次tradeoff，这是因为(1)其他小chunk发生内容变化时，牵连没变的chunk一起无法去重(2)如果在前面插入一个小chunk，那么整个Multi-chunk会发生偏移而无法去重。而这个n越大，去重率也就越低。

Our insight是这个规律在header blocks中并不适用。我们观察到header的修改时间字段（tar 存档视打包时的时间为修改时间）一定会发生变化，而且根据header内容计算的checksum也一定会发生变化。这从根本上导致了header参与到的chunk没有去重可能，

反而将临近的header聚合为Multi-metadata chunks具有存储优势，我们认为有以下原因：Tar存档中Header之前的相对顺序是按照name字段(路径名+文件名)排序的，这意味着如果我们将临近的Header聚合在一起作为一个chunk做lossless compression，它的路径名部分有很大可能被去冗余掉。此外用户名，组名，修改时间（tar 存档视打包时的时间为修改时间）等字段信息无论是否相邻都有较大概率重复。也就是说将多个header聚合为multi-Meta-chunk是在元数据开销与存储效率上达到双赢的设计，这一点在图\ref{Header}也可以得到验证。



==============

\paragraph{文件块}：对于小于 4 MiB 的文件，我们会将其数据块聚合为单个文件块。此策略可确保对未更改文件进行高效的重复数据删除，并将修改捕获到一个块中，从而优化元数据引导的增量压缩。

==============

\paragraph{CDC 块}（内容定义块）：对于大于 4 MiB 的文件，我们实现 FastCDC 来生成 CDC 块。此方法可在指定范围内保持块大小的一致性，从而通过防止块大小错位来提高增量压缩效率。


```