
**8KiB作为chunk平均大小：**

1. Zhu等人在"Avoiding the Disk Bottleneck in the Data Domain Deduplication File System"(FAST '08)中研究发现4-8KiB的chunk大小在多种数据集上提供良好的去重率
2. Meyer和Bolosky在"A Study of Practical Deduplication"(FAST '11)中分析了不同chunk大小对去重效率的影响，发现8KiB附近有较好的平衡点
3. EMC Data Domain系统采用8KiB平均chunk大小，如其技术白皮书所述

**4MiB容器设计：**

1. Lillibridge等人在"Sparse Indexing: Large Scale, Inline Deduplication Using Sampling and Locality"(FAST '09)中提出了容器概念，建议使用1-4MB
2. IBM Spectrum Protect(原Tivoli Storage Manager)在其去重实现中使用了类似大小的容器设计
3. Guo和Efstathopoulos在"Building a High-performance Deduplication System"(USENIX ATC '11)中讨论了容器设计与写入性能的关系
4. Srinivasan等人在"iDedup: Latency-aware, Inline Data Deduplication for Primary Storage"(FAST '12)中分析了容器大小对随机I/O影响

### Before Modified

```latex
Having established file boundaries (as illustrated in Figure~\ref{fig:sachunking}), we obtain numerous chunks of various types (File blocks, file header blocks, metadata header blocks). However, directly using these boundaries as final chunk delineations presents two significant challenges: Firstly, headers and other metadata chunks at only 512B are excessively small, substantially increasing the metadata overhead for the backup system. Secondly, excessively large files treated as single chunks may compromise system performance and delta compression efficiency. To address these challenges, we reorganize the data blocks into three distinct chunk types while preserving file boundaries, based on size and type:

The 4 MiB threshold represents an empirically determined optimal balance point between the advantages of each approach. Below this size, the metadata overhead of multiple chunks outweighs the potential deduplication benefits, while above this threshold, the advantages of granular content-defined chunking become significant enough to justify the additional processing complexity. Our approach maintains uniform chunk sizes (averaging 8 KiB, aligning with industry-standard practices in deduplication systems \cite{=========}) within a specified range, striking an optimal balance between metadata overhead and deduplication efficiency.


For files smaller than 4 MiB, we process the entire file as a single {\em File-chunk}. This approach perfectly aligns chunk boundaries with file boundaries, which directly fulfills our primary goal of eliminating boundary shifts. File-chunks provide several advantages: (1) simplified metadata management with one-to-one file-to-chunk mapping, (2) reduced processing overhead by avoiding complex chunking algorithms, and (3) optimized handling of small file modifications, as changes remain contained within a single chunk. Due to the alignment with file boundaries, delta compression can store only the actual changes to the file, eliminating redundant storage of unchanged content that would otherwise result from boundary shifts.

However, for very large files, this approach would create excessively large chunks that could negatively impact storage efficiency and processing performance.==============================

For files larger than 4 MiB, we implement FastCDC to generate {\em CDC-chunks} while preserving file boundaries. CDC-chunks offer distinct benefits for larger files: (1) they enable partial file deduplication even when modifications occur elsewhere in the file, (2) they maintain manageable chunk sizes that optimize compression algorithm efficiency, and (3) they distribute computational workload more evenly across the system. 

```


```latex
建立文件边界后（如图~\ref{fig:sachunking} 所示），我们获得了大量不同类型的数据块（文件块、文件头块、元数据头块）。然而，直接使用这些边界作为最终的数据块划分存在两个重大挑战：首先，只有 512B 的头块和其他元数据数据块过小，会大幅增加备份系统的元数据开销。其次，将过大的文件视为单个数据块可能会影响系统性能和增量压缩效率。为了应对这些挑战，我们在保留文件边界的同时，根据大小和类型将数据块重组为三种不同的数据块类型：

4 MiB 阈值代表了根据经验确定的每种方法优势之间的最佳平衡点。低于此大小时，多个数据块的元数据开销超过了潜在的重复数据删除优势；而高于此阈值时，基于内容的细粒度数据块划分的优势将变得足够显著，足以抵消额外的处理复杂性。我们的方法在指定范围内保持统一的块大小（平均为 8 KiB，与重复数据删除系统的行业标准做法一致 \cite{=========}），从而在元数据开销和重复数据删除效率之间取得最佳平衡。

对于小于 4 MiB 的文件，我们将整个文件作为单个 {\em File-chunk} 进行处理。这种方法完美地将块边界与文件边界对齐，直接实现了我们消除边界偏移的主要目标。File-chunk 具有以下几个优势：(1) 通过一对一的文件到块映射简化了元数据管理；(2) 通过避免复杂的分块算法降低了处理开销；(3) 优化了对小文件修改的处理，因为更改仍然包含在单个块中。由于与文件边界对齐，增量压缩可以仅存储文件的实际更改，从而消除了边界偏移导致的未更改内容的冗余存储。

然而，对于非常大的文件，这种方法会创建过大的块，从而对存储效率和处理性能产生负面影响。===============================

对于大于 4 MiB 的文件，我们实施 FastCDC 来生成 {\em CDC-chunks}，同时保留文件边界。CDC-chunks 对于较大的文件具有以下明显优势：(1) 即使文件其他位置发生修改，也能实现部分文件重复数据删除；(2) 它们保持可控的块大小，从而优化压缩算法效率；(3) 它们能够更均匀地在系统中分配计算工作负载。

4 MiB 阈值代表了根据经验确定的每种方法优势之间的最佳平衡点。低于此大小时，多个块的元数据开销超过了潜在的重复数据删除优势；而高于此阈值时，基于内容的细粒度分块的优势将变得足够显著，足以抵消额外的处理复杂性。我们的方法在指定范围内保持统一的块大小（平均为 8 KiB，与重复数据删除系统中的行业标准实践保持一致\cite{=========}），在元数据开销和重复数据删除效率之间取得最佳平衡。
```

### After modified



```latex
其次，将过大的文件视为单个数据块可能会影响系统性能和增量压缩效率。为了应对这些挑战，我们在保留文件边界的同时，根据大小和类型将数据块重组为三种不同的数据块类型：

我们的主要思想是保留文件边界，同时尽量将同一个文件的数据汇聚在一个chunk内，这对于去重或增量压缩的去冗余都是有帮助的(see challenge)。但一个过大的chunk会对增量压缩performance造成较大的负担，这是因为增量压缩本质上是拿base chunk的字符组合作为词组放入字典，并与target chunk去匹配。当chunk size过大时，字典里内容过多，匹配开销过大。所以我们需要设置一个阈值来界定大小文件，以便在遇到大文件时做额外的处理。我们设置的阈值是与备份系统容器大小相同（默认为4MiB，该阈值作为容器大小是多个研究的共识/cite{======}），以保证我们认定的小文件 size 不会超过备份系统容器。


对于小于 4 MiB 的文件，我们将整个文件作为单个 {\em File-chunk} 进行处理。这种方法完美地将块边界与文件边界对齐，直接实现了我们消除边界偏移的主要目标。File-chunk 具有以下几个优势：(1) 通过一对一的文件到块映射，增大了平均chunk size，降低了元数据管理开销；(2) 通过避免复杂的分块算法（特别是计算滚动哈希值的过程）降低了计算开销；(3) 优化了对小文件修改的处理，因为更改仍然包含在单个块中。由于与文件边界对齐，增量压缩可以仅存储文件的实际更改，从而消除了边界偏移导致的未更改内容的冗余存储。

对于大于 4 MiB 的文件，我们在保留文件边界的基础上，在大文件内部实施 FastCDC 来生成 {\em CDC-chunks}。这并不意味着对大文件的处理我们等同于纯粹的FastCDC，这是因为我们先确保了整个大文件的开始点与结束点两个边界，这避免了跨文件的级联偏移影响。将较大的文件处理为CDC-chunks具有以下优势：(1) 即使文件其他位置发生修改，也能实现部分文件重复数据删除；(2) 它们保持可控的块大小，从而优化增量压缩算法效率；


```
