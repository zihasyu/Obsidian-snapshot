#### 总结：
saving提升缘由：比起Feature-based方法，多匹配到了一些相似块（如case3），做了很多delta compression

problem：当在做File-level极致化去冗余时，我们期待一个文件修改后能匹配到它修改前的版本并基于其做delta compression，但Feature-based方法虽然能保证匹配到相似块的相似性，却不能保证每次都能匹配到它修改前的版本。

wrting: 想从**在文件发生修改时，我们期待修改后的文件匹配到修改前的同名文件以做增量压缩，但Feature-based会有下图中case3这种没能匹配到的情况**这个问题出发。
这个问题出发的话，遗憾的是没能从design1导致的问题引到新问题，而是两个相对并列的问题。
两个问题的联系是：① 第一个问题（SC那个问题）需要用File-level chunking解决，第二个问题在File-level中才明显（因为chunk-level不存在修改前的同名chunk这个概念）② 这两个问题的解决都用到了Header里的metadata
#### 问题分析
![[Pasted image 20250512154825.png]]

在用Feature-based方法做文件级别的增量压缩时，我们expect两种匹配情况：

（1）**基于一个文件为模板新建文件时，expect新建的文件匹配到模板文件**。他能给新建的文件（及其后续版本）只存储与模板文件有区别的数据。
（see File B -> File A， File B' -> File A）。我们称之为**case 1**。

（2）**文件发生修改时，expect修改后的文件匹配到修改前的同名文件**。当一个文件发生修改时，正如增量压缩的理念只存储修改量，一般来讲最适合它的base file会是修改前的该文件(see File A' -> File A, File C' -> File C)，我们称之为**case 2**。
但因为Feature-based方法完全基于内容且对相似度的要求较高，所以会出现即便修改得不多，但也无法匹配而损失压缩机会的情况(File D' No match File D)，我们称之为**case 3**。


接下来我们逐个对case分析是否存在问题：
- case 1: 只有Feature-based方法能匹配到，我们认为这是Feature-based的优势区间，不存在问题。
- case 2: Feature-based方法虽然能匹配到，但design 1会做出file-level的chunk，而对整个File算特征值是**performance较差**的方案，如果跳过计算特征值直接匹配同名文件，则会省去正部分算特征值的时间开销。
- case 3: Feature-based方法未能匹配到，**损失了一部分saving**，且即便没匹配到还是会算特征值以**损失performance**。
接下来需要实验证明case2 case3占比较多，来argue Feature-based问题较为严重。

#### design2-motivation实验支撑

用SA.Odess匹配到的依赖关系  与SA.Meta匹配到的依赖关系进行对比
（SA.Meta并非我们最终含false filter的方法，而是最straight forward的如上图中一样的 file name匹配，无论压缩率如何都直接接受）
![[Excalidraw/Drawing 2025-05-12 11.11.07.excalidraw.md#^group=x4MJoao_wtk6etX7JDsHP]]

| Dataset   | case 1    | case 2  | case 3    | case 1 % | case 2 % | case 3 % |
| --------- | --------- | ------- | --------- | -------- | -------- | -------- |
| linux     | 185,279   | 656,628 | 1,039,000 | 9.8%     | 34.8%    | 55.3%    |
| WEB       | 473,198   | 7,684   | 402,384   | 53.6%    | 0.9%     | 45.6%    |
| chromium  | 1,053,592 | 11,010  | 1,866,447 | 35.9%    | 0.4%     | 63.7%    |
| automake  | 12,419    | 8,973   | 8,504     | 41.6%    | 30.1%    | 28.5%    |
| bash      | 2,086     | 5,129   | 6,454     | 15.3%    | 37.5%    | 47.2%    |
| coreutils | 2,178     | 3,417   | 4,630     | 21.3%    | 33.4%    | 45.3%    |
| fdisk     | 88        | 138     | 196       | 20.8%    | 32.7%    | 46.5%    |
| glibc     | 79,310    | 102,157 | 111,608   | 27.1%    | 34.9%    | 38.0%    |
| smalltalk | 1,001     | 3,692   | 4,431     | 11.0%    | 40.5%    | 48.5%    |
| GCC       | 193,560   | 167,162 | 473,448   | 23.2%    | 20.0%    | 56.8%    |

case2的情况普遍较多，对此优化能提升performance，
每个数据集上case3的情况更多，对此优化可以进一步提升saving和performance。

#### design2

我们原本的design2过于straight forward，那将这个straight forward的方案作为优化case 2和case 3的一个baseline方法（见下图）：

通过提取header中的name字段，建立index用于匹配。但是考虑到每个name字段有100字节，对于index开销来说过大，我们对name求了哈希值降低至8字节，以该name的hash值为key查询NameHash Index用于匹配修改前的同名文件。


![[Metadata-guided 1.png]]

这个方法遇到的问题是，我们固然通过Metadata-guided匹配到了更多的相似块，但它会提前扼杀压缩潜力，想想一下，当文件A修改过大产生A'时，比起A'及其以后的版本都基于A去做增量压缩，不如在侦测到较大的修改时将A'作为新的basechunk，替换掉NameHash中原本A的位置。这样做后续的A'',A'''...就会基于A'去做增量压缩。

因为备份系统是一个在线系统，用户后续还会长期使用，所以考虑到长期收益，我们不能滥用metadata-guided detection（如果滥用，那么后期的压缩效果还不如SA.Odess），于是我们的方案引入一个基于统计数据的false filter用于保证长期收益，同时这样完善了两模式切换的条件，不再是最直接的metadata-guided匹配不到时就去用Feature-based detection。

![[FalseFilter.png]]
#### evaluation结果支撑原因
##### 证明1：匹配到更多相似块，提升了saving
SA.FineTar DCC相较于SA.Ode的DCC有较大的提升，说明我们的方法匹配了更多的相似块，以及最后saving也有提升。

1. DCC:unique chunk中 delta chunk所占的比例。
2. Ode.是odess的缩写
3. SA是design1的缩写

| 数据集-DCC            | MTar.Ode<br>DCC | SA.Ode<br>DCC | SA.FineTar<br>DCC<br>(对比SA.Odess提升的百分比) |
| ------------------ | --------------- | ------------- | --------------------------------------- |
| linux              | 0.482372        | 0.419918      | 0.75 (+78.61%)                          |
| WEB                | 0.722273        | 0.524831      | 0.817814 (+55.82%)                      |
| chromium           | 0.323427        | 0.32396       | 0.873972 (+169.78%)                     |
| automake           | 0.419124        | 0.658459      | 0.831415 (+26.27%)                      |
| bash               | 0.478422        | 0.459818      | 0.732522 (+59.31%)                      |
| coreutils          | 0.407394        | 0.476779      | 0.785939 (+64.84%)                      |
| fdisk              | 0.36826         | 0.338323      | 0.594311 (+75.66%)                      |
| glibc              | 0.406422        | 0.573923      | 0.787196 (+37.16%)                      |
| smalltalk          | 0.399104        | 0.415494      | 0.665339 (+60.13%)                      |
| GCC                | 0.407066        | 0.375009      | 0.797803 (+112.74%)                     |
| ~~Android~~        | 0.0257647       | 0.056418      | 0.74969 (+1228.97%)                     |
| ~~ThunderbirdTar~~ | 0.0378863       | 0.0435636     | 0.725785 (+1566.08%)                    |
DCC这个数据可以摆evaluation，用于佐证解决第二个问题后，能匹配到大量相似块。


同时观察SA.Ode与MTar.Ode的DCC对比（有的数据集MTar更高，有的数据集SA更高），所以我们不能说在适用第一个design之后，会造成不利于匹配相似块的因素。
这也是为什么上文从**文件发生修改时，expect修改后的文件匹配到修改前的同名文件，但Feature-based会有case3这种情况没能匹配到**这个问题出发。

这个问题出发的话，遗憾的是没能从design1导致的问题引到新问题，而是两个相对并列的问题。
两个问题的联系是：① 第一个问题需要用File-level chunking解决，第二个问题在File-level中才明显（因为chunk-level不存在修改前的同名chunk这个概念）② 这两个问题的解决都用到了Header里的metadata

| 数据集-Overall Compression Ratio | Fast.Ode.   | Mtar.Ode.   | SA.Ode      | SA.FineTar  |
| ----------------------------- | ----------- | ----------- | ----------- | ----------- |
| linux                         | 37.4756     | 55.0854     | 73.4865     | 84.0509     |
| chromium                      | 84.7032     | 112.718     | 120.126     | 128.448     |
| WEB                           | 92.1916     | 227.162     | 233.704     | 245.958     |
| automake                      | 13.9886     | 18.7928     | 20.7204     | 23.795      |
| bash                          | 9.05485     | 9.23801     | 9.07142     | 10.4203     |
| coreutils                     | 11.4398     | 12.1444     | 15.3239     | 18.3285     |
| fdisk                         | 8.6656      | 8.71723     | 10.606      | 11.6088     |
| glibc                         | 31.6662     | 44.4125     | 46.6012     | 54.0683     |
| smalltalk                     | 23.5628     | 27.1787     | 31.5137     | 34.1479     |
| GCC                           | 19.4913     | 27.0188     | 30.0167     | 35.6228     |
| ~~ThunderbirdTar-log~~        | ~~6.60713~~ | ~~6.60889~~ | ~~6.81257~~ | ~~8.67753~~ |
| ~~Android-log~~               | ~~6.49222~~ | ~~6.50743~~ | ~~7.90553~~ | ~~10.3117~~ |
##### 证明2：false filter控制长期收益的优势

红线是有false filter的方案，深蓝线是最straight forward的方案。（后面换成没有点的，完整数据的曲线）
![[Pasted image 20250512181335.png]]

早期几个版本含false filster的方案overall compression ratio不如全接受的方案，但随着版本增多会逐渐体现出优势。




#### 让大文件处理不straight forward的处理：

![[Metadata-guided 1.png]]

原本做法是将大文件内部的chunks直接拿去做Feature-based detection，没能用上metedata-guided，这里的原因是大文件内部cdc-chunks过多（它们共用同一个namehash-key），总不可能一个个遍历压缩过去，那么确实只有通过feature-based detection方案是符合性能预期的。

cdc-chunks用Feature-based方法其实共会匹配到0-3个相似块，并使用随机一个匹配到的相+似块去做增量压缩（出于性能考量，传统方法不会考虑全压缩一遍再取最优压缩率的结果，而是随机选一个，或者直接选择第一个匹配到的相似块）。

考虑到前面提到的problem，我们肯定更希望修改后的文件能匹配到修改前的版本，所以当cdc-chunks用Feature-based detection匹配到复数相似块时，我们用metadata-guided detection对比一遍，优先选择这复数相似块中 共有的chunk作为base chunk。


![[Pasted image 20250515105443.png]]

