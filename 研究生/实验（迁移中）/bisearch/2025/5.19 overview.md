semantics-aware chunking 强调了两个点：① 与MTar最大的区别，我们的chunk边界与文件边界对齐 ② header太小，单独作为一个chunk元数据管理开销太大。MTar的做法是重构文件并在重构好的文件（chunking前，restore之后，两次重构都要大量时间）上做cdc来规避了这个问题。 但因为header具有不重复性，我们认为在其上做cdc意义不大，we propose fixed-length aggregation 即可 → 不需要重构文件

通过语义感知分块解决了级联偏移挑战，其主要生成具有对齐边界的文件级块，我们将注意力转向提高这些文件对齐块的增量压缩（匹配相似块）。
我们发现依靠传统Feature-based方法，存在较多（28.5-63.7\%）的pre-modification version mismatch情况（target chunk存在历史版本，但它没有匹配到任何相似块）。
为能给File-level chunk提供他们pre-modification version的相似块匹配选项，我们提出了metadata-guided， 它提供了一种能利用Tar里面的元数据来有效匹配pre-modification version的方案，用于补充相似块匹配的选项。


元数据引导匹配（metadata-guided matching）虽然能改进相似块匹配能力，但盲目地对所有元数据匹配的文件应用增量压缩需要谨慎考虑。我们观察到：
1. 虽然最大化增量压缩操作数量看似有益，但这种方法只优化短期压缩收益
2. 在增量压缩实现中，为了性能考虑，通常是对基础块进行编码，而不是对其他已增量压缩的块
3. 当文件经历多次连续修改时，每个版本都会对原始基础块进行增量压缩，而不是对其直接前任版本
→→所以我们的观点是如果中间版本经历了实质性修改，将其存储为新的基础块可能会为后续版本提供更好的长期压缩效果。
为解决这个问题，我们引入了动态评估机制，基于多种指标评估每个增量压缩机会。在后文中，将演示"误检过滤器"（False Detection Filter）如何有选择地仅在提供有意义存储收益时应用增量压缩，确保某些版本能作为未来修改的更好基础块。