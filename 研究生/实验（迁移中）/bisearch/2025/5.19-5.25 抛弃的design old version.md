

这种限制的出现是因为块的标识符都完全基于chunk内容。  
用于为target chunk匹配相似块的的标识符为特征值，即使内容发生轻微变化，相应的特征值也有可能发生改变。  
我们的分析表明，用sachunking后，基于特征值的方法会有xx%-xx%的unique chunk在明确存在pre-modification的前提下未能匹配到相似块（详见第~\ref{sec:evaluation} 节），这些chunk作为base chunk做了无损压缩，损失了增量压缩机会。


待修改：
1. [x] motivation把tar archive介绍中加上少量存在的其他条目（而非只有file entry）
2. [x] Multi-Meta-chunk 改名叫 Header-Aggregate-chunk，此外要同步于ppt的图才行。此外用size字段的地方要加上向上取整
### metadata-guided2
```
我们期待于每个chunk修改时，都能为它匹配到修改前的版本作为basechunk进行增量压缩，但是这个是无法做到的。这是因为chunk唯一性的标识符为fp不能作用于相似块匹配（回想一下，只有chunk完全重复时才具有相同的fp），Feature-based方法能做到部分预期的效果，但始终有较多chunk无法匹配到修改前的版本（数据参考于ref{}）。所以我们希望有一个不随着chunk内容改变的特殊标识符来为chunk匹配到修改前版本。
于是我们提出了metadata-guided resemblance detection。我们希望通过文件名来实现这一点。我们发现，tar包中的文件名在不同版本之间是保持一致的，这就为我们提供了一个不随着chunk内容改变而一直存在的特殊标识符来实现这一点。
```



```
\subsection{Metadata-guided Resemblance Detection}
\label{subsec:metadata-guided}

We observe that traditional feature-based resemblance detection methods often fail to recognize version relationships between files that have undergone substantial modifications. Through our experimental analysis across multiple datasets, we identify that these pre-modification version mismatches account for 28.5-63.7\% of all potential matching scenarios (see Table~\ref{tab:matching-cases} in \S\ref{sec:evaluation}). These missed opportunities occur when feature-based detection fails to match files with their previous versions, despite clear semantic relationships between them.

When employing feature-based methods for file-level delta compression, we encounter two primary matching scenarios (Fig.~\ref{fig:deltamotivation}): (i) \textbf{File creation from templates}, where newly created files match their template files, allowing storage of only differences between the new file and the template; and (ii) \textbf{File modification}, where a file's most suitable base would be its pre-modification version. However, we find that feature-based methods often fail in the second scenario, as even moderate modifications can prevent successful matching when relying solely on content similarity with high thresholds.

\begin{figure}[t]
    \centering
    \includegraphics[width=3.4in]{pic/design/Metadata-guided.png}
    \vspace{-8pt}
    \caption{Metadata-guided Resemblance Detection architecture: Prioritizing efficient metadata matching with feature-based detection as secondary approach.}
    \vspace{-12pt}
    \label{fig:Metadata-guided}
\end{figure}

\subsubsection{Dual-modal Resemblance Detection Approach}

We address these limitations by proposing a dual-modal resemblance detection system in FineTAR that combines metadata guidance with conventional feature-based methods. We argue that filename consistency across backup versions provides a reliable indicator of file relationships, regardless of content changes. This approach specifically targets the "pre-modification version mismatch" problem, enabling us to identify version relationships when content features fail to recognize heavily modified files.

Benefiting from our semantic-aware chunking that effectively extracts and preserves metadata, FineTAR leverages the rich metadata information naturally present in tar archives to enable rapid identification of potential similar chunks without the computational overhead of content analysis. We identify two key advantages of this approach: (1) metadata-based matching can detect similarities even when content modifications invalidate traditional feature values, and (2) it requires significantly fewer computational resources than generating and comparing content hashes.

\subsubsection{Implementation for Different Chunk Types}

We implement metadata-guided detection differently across chunk types based on their characteristics:

\begin{itemize}
\item \textbf{For File-chunk:} We extract path and filenames from associated headers and compute hash values for the nameHash-index. When processing a unique File-chunk, we first query this index to identify chunks with matching names from previous backups. This approach leverages our observation that files retaining identical names across backup versions typically exhibit high content similarity, enabling efficient delta compression without feature extraction costs. We fall back to traditional feature-based detection only when no matching names are found.

\item \textbf{For Multi-Meta-chunk:} We truncate up to n (default=2) trailing path components from one header within each Multi-Meta-chunk, compute a hash value, and record it in the nameHash-index. For incoming unique Multi-Meta-chunks, we first attempt to identify matching patterns in the metadata index before resorting to feature-based methods.

\item \textbf{For CDC-chunks:} We recognize that large files produce multiple CDC-chunks, making one-to-one filename mapping impractical. Therefore, we apply feature-based detection directly to identify similar chunks based on content characteristics.
\end{itemize}

\subsubsection{Balancing Metadata-guided and Feature-based Approaches}

We combine metadata-guided and feature-based approaches as complementary methods with distinct advantages. We find that metadata-guided detection effectively identifies similarities despite content modifications, particularly for files maintaining their names despite changes. Additionally, it significantly reduces computational overhead by eliminating redundant feature calculations when metadata matching succeeds.

Conversely, we leverage feature-based detection for identifying similar chunks regardless of names, which proves valuable when processing files with different names but similar content. Given these complementary strengths, we employ a sequential strategy: first attempting metadata-guided detection for efficiency and lower computational cost, then falling back to feature-based detection when necessary.

We address the potential issue of suboptimal results from metadata matching through a false detection filter (Section~\ref{subsec:falsefilter}). We recognize that metadata similarity does not guarantee content similarity, so our filter evaluates delta-compressed chunk quality and dynamically adjusts the detection strategy based on historical performance, ensuring optimal compression efficiency while maintaining high throughput across diverse backup workloads.
```

### metadata-guided1
```
\subsection{Metadata-guided Resemblance Detection}

\label{subsec:metadata-guided}

  

%%%% (1) pre-modification version mismatch -->

  
  

We identify that traditional feature-based methods frequently fail to recognize version relationships when files undergo substantial modifications, resulting in significant compression opportunities being missed. To address these limitations, we propose a dual-modal resemblance detection system in FineTAR that combines metadata guidance with conventional feature-based methods. We argue that filename consistency across backup versions provides a reliable indicator of file relationships, regardless of content changes. This approach particularly addresses the "pre-modification version mismatch" problem discussed earlier, where we can leverage metadata to identify version relationships when content features fail to recognize heavily modified files.

  

\begin{figure}[t]

    \centering

    % \includegraphics[width=3.4in]{pic/sachunking.pdf}

    \includegraphics[width=3.4in]{pic/design/Metadata-guided.png}

    \vspace{-8pt}

    \caption{Metadata-guided Resemblance Detection architecture: Prioritizing efficient metadata matching with feature-based detection as secondary approach.}

    \vspace{-12pt}

    \label{fig:Metadata-guided}

\end{figure}

  
  

Benefiting from the semantic-aware chunking that effectively extracts and preserves metadata, FineTAR introduces a metadata-guided resemblance detection method to enhance delta compression efficiency. By leveraging the rich metadata information naturally present in tar archives, this innovative approach enables rapid identification of potential similar chunks without the computational overhead of content analysis. The key advantage is that metadata-based matching can detect similarities even when content modifications would invalidate traditional feature values, while requiring significantly less computational resources than generating and comparing content hashes.

  

To handle cases where metadata matching alone is insufficient, FineTAR incorporates a complementary feature-based detection method. Different chunk types utilize metadata-guided detection in varying ways based on their characteristics, with feature-based detection serving as a fallback mechanism when necessary. Below, we detail how each chunk type leverages metadata information and when the system switches to traditional feature-based approaches:

\begin{itemize}

\item \textbf{For File-chunk:} FineTAR extracts path and filenames from their associated headers and computes hash values for storage in the nameHash-index. When processing a unique File-chunk, the system first queries this index to identify chunks with matching names from previous backups. This metadata-based approach leverages the observation that files retaining identical names across backup versions typically exhibit high content similarity, enabling efficient delta compression without the computational cost of feature extraction. If no matching names are found, the system falls back to traditional feature-based detection.

  

\item \textbf{For Multi-Meta-chunk:}  The system truncates up to n (default=2) trailing path components from one header within each Multi-Meta-chunk, computes a hash value, and records it in the nameHash-index. (If the path depth is insufficient, fewer or no components are truncated to maintain meaningful path information.) For incoming unique Multi-Meta-chunks, FineTAR first attempts to identify matching patterns in the metadata index before resorting to feature-based methods when necessary.

  

\item \textbf{For CDC-chunks:} Since large files produce multiple CDC-chunks, maintaining one-to-one filename mapping becomes impractical. Therefore, FineTAR directly applies feature-based detection to identify similar chunks based on content characteristics.

\end{itemize}

  

In resemblance detection, we combine metadata-guided and feature-based approaches, as each method has its distinct advantages. Metadata-guided detection can effectively identify similarities even when content modifications would invalidate traditional feature values, which is particularly beneficial for files that maintain their names despite undergoing changes. Additionally, it significantly reduces computational overhead by eliminating redundant feature calculations when metadata matching succeeds.

  

Conversely, feature-based detection excels at identifying similar chunks regardless of their names, which is particularly valuable when processing backup data containing files with different names but similar content. Given these complementary strengths, FineTAR employs a sequential strategy: first attempting metadata-guided detection for its efficiency and lower computational cost, then falling back to feature-based detection when necessary to ensure comprehensive coverage of similarity detection.

  

To address cases where metadata matching produces suboptimal results (as metadata similarity does not guarantee content similarity), FineTAR incorporates a false detection filter (Section~\ref{subsec:falsefilter}) that evaluates the quality of delta-compressed chunks. This mechanism dynamically adjusts the detection strategy based on historical performance, ensuring optimal compression efficiency while maintaining high throughput across diverse backup workloads.
```

### sachunking3
```
%%  when we apply cdc in the file regions and header regions, we'll get more small chunks, which will increase the metadata overhead.

Having established file boundaries (as illustrated in Figure~\ref{fig:sachunking}), we obtain numerous chunks of various types. However, directly using these boundaries as final chunk delineations presents two significant challenges: Firstly, headers at only 512B are excessively small, substantially increasing the metadata overhead for the backup system. Secondly, applying CDC across all file regions would further reduce average chunk size, exacerbating metadata management overhead - this motivates our decision to treat smaller files as single chunks. Meanwhile, excessively large files treated as individual chunks may compromise system performance. To address these challenges, we reorganize the data blocks into three distinct chunk types while preserving file boundaries, based on size field and type field:

```


sachunking2
```
%%%% size

We propose a semantics-aware chunking approach that aligns chunk boundaries with file boundaries by leveraging the inherent structure of tar archives. In tar format, files and their metadata are organized in a well-defined structure where file boundaries can be precisely identified by parsing specific fields in the headers and their encapsulated metadata.

Each file is preceded by a file header block (header that manages file metadata; we refer to these as "file headers" to distinguish them from other header types in the tar format). By parsing the {\em size} field in these file headers (as shown in Figure~\ref{fig:sachunking}), we can determine exact file boundaries. For example, using the file header's beginning as a reference point {\em H}, the file header's boundary would be defined as {\em (H, H+512)}, the file's boundary would be defined as {\em (H+512, H+512+size)}.

  

%%%%type

Additionally, tar archives primarily consist of 512B file header blocks and variable-length files, but also include special 512B metadata header blocks that contain only metadata (such as directories, symbolic links, and device files) which, unlike file headers, are not followed by file content.

Without distinguishing between file header and metadata header, we would extract incorrect file boundaries (metadata header's {\em size} field is 0), potentially causing out-of-bounds errors. However, by utilizing the {\em type} field, we effectively resolve this issue (we only use the boundary judgment scheme mentioned above when we determine that the file header is a file header. If it is a metadata header, we only process the boundary of the current metadata header as {\em (H, H+512)}, and then parse the size and type of the next header).

  

Having established file boundaries (as illustrated in Figure~\ref{fig:sachunking}), we obtain numerous chunks of various types (File blocks, file header blocks, metadata header blocks). However, directly using these boundaries as final chunk delineations presents two significant challenges: Firstly, headers and other metadata chunks at only 512B are excessively small, substantially increasing the metadata overhead for the backup system. Secondly, excessively large files treated as single chunks may compromise system performance and delta compression efficiency. To address these challenges, we reorganize the data blocks into three distinct chunk types while preserving file boundaries, based on size and type:

\paragraph{Multi-Meta-chunk}: For file header blocks and metadata header blocks, which uniformly occupy 512B each, processing individual headers as separate chunks would impose substantial metadata overhead on the system. Our insight is to aggregate multiple headers (default: 16 headers, aligning with the common 8KiB average chunk size in backup systems) into a single Multi-Meta-chunk for both deduplication processing (lossless or delta compression) and metadata management.

  

In conventional scenarios, merging multiple small chunks into a single chunk typically represents a tradeoff between storage efficiency and metadata overhead. This occurs because: (1) content changes in any constituent small chunk prevent deduplication of the entire aggregated chunk, including unchanged components, and (2) insertion of a small chunk at the beginning shifts all subsequent content, preventing deduplication of the entire Multi-chunk. Generally, as n (number of aggregated chunks) increases, deduplication efficiency decreases.

  

However, our insight is that this pattern does not apply to header blocks. We observe that the modification time field in headers (tar archives treat packaging time as modification time) invariably changes between backup versions, and consequently, the checksum calculated from header content also changes. This fundamentally eliminates any possibility of deduplication for chunks containing headers.

  

Conversely, aggregating adjacent headers into Multi-Meta-chunks offers storage advantages for several reasons. Headers in tar archives are ordered according to the name field (pathname + filename), meaning that when adjacent headers are aggregated into a single chunk for lossless compression, the pathname components have a high probability of being deduplicated. Additionally, username, group name, and modification time fields have significant repetition probability regardless of adjacency. In other words, aggregating multiple headers into Multi-Meta-chunks achieves a win-win design between metadata overhead reduction and storage efficiency, a finding validated by the results presented in Figure~\ref{fig:MultHeader}.

  

\paragraph{File-chunk and CDC-chunks}: Our key insight is to preserve file boundaries while consolidating data from the same file into a single chunk whenever possible, which benefits both deduplication and delta compression redundancy elimination (see challenges in \S\ref{subsec:motivation}). However, excessively large chunks can significantly burden delta compression performance, as delta compression essentially builds a dictionary of character combinations from the base chunk to match against the target chunk. When chunk size grows too large, the dictionary becomes unwieldy, increasing matching overhead. Therefore, we establish a threshold to differentiate between small and large files, enabling special handling for larger files. We set this threshold at 4 MiB, which aligns with the container size commonly accepted in backup systems \cite{zhu08}, ensuring our designated "small files" never exceed backup system container capacities.

  

For files smaller than 4 MiB, we process the entire file as a single {\em File-chunk}. This approach perfectly aligns chunk boundaries with file boundaries, which directly fulfills our primary goal of eliminating boundary shifts. File-chunks provide several advantages: (1) simplified metadata management with one-to-one file-to-chunk mapping, increasing average chunk size and reducing metadata overhead; (2) reduced processing overhead by avoiding complex chunking algorithms (particularly the computation-intensive rolling hash calculation); and (3) optimized handling of small file modifications, as changes remain contained within a single chunk. Due to the alignment with file boundaries, delta compression can store only the actual changes to the file, eliminating redundant storage of unchanged content that would otherwise result from boundary shifts.

  

For files larger than 4 MiB, we implement FastCDC to generate {\em CDC-chunks} while preserving file boundaries. This does not mean our approach for large files is equivalent to pure FastCDC implementation, as we first ensure both starting and ending boundaries of the entire large file, preventing cross-file cascading boundary shifts. Processing larger files as CDC-chunks offers distinct benefits: (1) enabling partial file deduplication even when modifications occur elsewhere in the file; (2) maintaining manageable chunk sizes that optimize delta compression algorithm efficiency.
```



sachunking1
```
% To ensure file boundaries align with chunk boundaries, we propose a Semantics-aware Chunking method that utilizes tar header metadata to determine chunk boundaries. The process begins by sequentially reading and analyzing tar file headers. Following the UNIX POSIX Tar specification, we extract metadata fields (name, size, and type, As shown in the Header block section of Figure~\ref{fig:sachunking}) based on header offset positions. Our method calculates the ending offsets of both header and data blocks to establish file semantic boundaries, creating chunks that preserve header context while separating headers from their associated data blocks.

  

% For the following data blocks, we generate three kinds of chunks based on their file size and type as follows:

  

% \begin{itemize}

%     \item \textbf{File-chunks}:  For files smaller than 4 MiB, we aggregate their data blocks into a single File-chunk. This strategy ensures efficient deduplication of unchanged files and captures modifications into one chunk, optimizing metadata-guided delta compression.

%     \item \textbf{Multi-Meta-chunks}: For special file entries (directories, symbolic links, and device files) containing only metadata, we generate Multi-Meta-chunks from their headers, preserving essential information while minimizing storage overhead.

%   \item \textbf{CDC-chunks} (Content-Defined chunks): For files larger than 4 MiB, we implement FastCDC to generate CDC-chunks. This approach maintains uniform chunk sizes within a specified range, enhancing delta compression efficiency by preventing chunk size misalignment.

% \end{itemize}

  
  

% Figure \ref{fig:sachunking} provides a clear visual representation of our Semantic-aware Chunking approach. This innovative method addresses the boundary instability problem in tar backup systems by utilizing file semantic information rather than content patterns to determine chunk boundaries.

  

% As illustrated in the figure, our approach fundamentally differs from traditional content-defined chunking in handling file modifications. When a file changes (File 2 in the illustration), traditional CDC methods (upper portion) trigger a cascade of boundary shifts that propagate to subsequent unmodified files (File 3), significantly reducing deduplication efficiency across backup versions. In contrast, our Semantic-aware Chunking (lower portion) maintains stable file boundaries by anchoring chunk delineation to file semantic information extracted from tar headers.

  

% The figure effectively demonstrates how our method processes the tar archive stream by precisely identifying file boundaries through metadata analysis. By leveraging header information to determine chunk boundaries, we create a more stable chunking foundation that preserves semantic file relationships while preventing cross-file boundary shifts. This approach directly addresses the 36.0\% shift chunk ratio problem observed in our motivation study, ensuring that modifications to one file remain isolated and do not affect the chunking of other unchanged files.

  

% This semantic preservation is particularly valuable for tar backups where logical file boundaries carry significant meaning but are often ignored by traditional content-based approaches. By aligning chunking boundaries with file semantics, we maximize deduplication opportunities while maintaining system performance through efficient metadata processing.

  

% This semantic-aware approach directly addresses the boundary unstability problem observed in our motivation study by ensuring that file boundaries are preserved, preventing the propagation of boundary shifts between files.

  

%% pseudocode & its description

  

% \begin{algorithm}[!ht]

%   \caption{SA Chunking}

%   \begin{algorithmic}[1]

%       \Function{SAChunking}{$src, end$}

%       \State $Type \gets HEADER$

%       \While {$src < end$}

%       \If {$Type = HEADER$}

%       \State $Type \gets src[156]$

%       \State $size \gets 0$

%       \For {$i = 0$ to $10$}

%       \State $size \gets size \times 8 + src[124 + i]$

%       \EndFor

%       \State $size \gets \left\lceil \frac{size}{512} \right\rceil \times 512$

  

%       \If {$Type = FILE$ and $size \geq 4MiB$}

%       \State $Type \gets BIGFILE$

%       \EndIf

%       \State Append to header buffer

%       \State when the buffer is full, pushes the buffer

%       \State $src \gets src+512$

%       \ElsIf {$Type = FILE$}

%       %\State Allocate memory for file chunk

%       \State $Type \gets HEADER$

%       \State $src \gets src+size$

  

%       \ElsIf {$Type = BIGFILE$}

%       \State $cpSum \gets 0$

%       \While {$cpSum<size$}

%       \State $cpSum += FastCDC(src+cpSum)$

%       %\State Allocate memory for CDC chunk

%       \EndWhile

%       \State $Type \gets HEADER$

%       \State $src \gets src+size$

%       \Else

%       \State Append to header buffer

%       \State when the buffer is full, pushes the buffer

%       \State $Type \gets HEADER$

%       \State $src \gets src+512$

%       \EndIf

%       \EndWhile

%       \EndFunction

%   \end{algorithmic}

%   \label{SA-algorithm}

% \end{algorithm}

  

% Figure \ref{fig:sachunking} illustrates our Semantics-aware Chunking process. The algorithm takes the source data ($src$) and its end position ($end$) as input. For each iteration, it first processes a header block by extracting the file type and size information from specific offset positions (lines 4-11). Based on the file type and size, it categorizes the data into different chunk types: regular files (FILE), large files (BIGFILE), or metadata-only entries. For header blocks, the algorithm accumulates them in a buffer until reaching 8KiB (16 blocks) before generating a Multi-Meta-chunk (lines 13-15). For regular files smaller than 4MiB, it processes the entire file as a single File-chunk (lines 16-18). For files larger than 4MiB, it applies FastCDC to generate variable-sized chunks within the file's data region (lines 19-24). For special file types containing only metadata, it processes them similarly to header blocks (lines 25-29).
```