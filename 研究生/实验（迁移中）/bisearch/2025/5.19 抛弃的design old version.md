
### For False Filter

```
\subsection{False Detection Filter}
\label{subsec:falsefilter}

We argue that while metadata-guided resemblance detection significantly improves performance, blindly applying delta compression to all metadata-matched chunks requires careful consideration. Although maximizing delta compression coverage seems beneficial, we observe that this approach optimizes only for short-term compression gains. In delta compression implementations, chunks are typically encoded against base chunks (not against other delta-compressed chunks) for performance reasons. Consequently, when a chunk undergoes several sequential modifications, each version would be delta-compressed against the original base chunk rather than its immediate predecessor. This mechanism evaluates the long-term effectiveness of delta compression decisions, focusing on two fundamental aspects:

\begin{figure}[t]
    \centering
    \includegraphics[width=3.4in]{pic/design/FalseFilter.png}
    \vspace{-8pt}
    \caption{FalseFilter workflow: Identifying suboptimal delta compression results using compression metrics to optimize long-term storage efficiency.}
    \vspace{-12pt}
    \label{fig:FalseFilter}
\end{figure}

1. \textbf{Quality of Delta Compression Operations}: We identify that metadata-based detection, while efficient, may occasionally produce suboptimal Delta Compression Ratio (DCR), affecting overall compression efficiency.

2. \textbf{Long-term Impact on Storage Efficiency}: We observe that when a chunk undergoes delta compression, it typically becomes ineligible as a base chunk for other chunks. If an intermediate version undergoes substantial modifications, we argue that storing it as a new base chunk may yield better long-term compression effectiveness for subsequent versions.

To demonstrate this long-term impact, we examine different resemblance detection strategies in backup systems. Our analysis reveals that systems prioritizing higher Delta Compression Coverage (DCC) often exhibit inferior overall compression ratios after processing multiple backup versions. This indicates that maximizing delta compression coverage without considering the quality of base chunks can lead to suboptimal long-term storage efficiency, particularly when valuable potential base chunks are converted into delta chunks.

To address this challenge, we develop a dynamic evaluation mechanism that assesses each delta compression opportunity based on multiple metrics:

1. LZ4 Compression Ratio (LZ4Ratio):
\begin{equation}
    \label{eq:lz4Ratio}
    \small
    \normalsize
    LZ4Ratio = \frac{1}{BChunkNum} \sum_{i=1}^{BChunkNum} Ratio_{LZ4}(Chunk_i)
\end{equation}
where $BChunkNum$ represents the total number of current base chunks, and Ratio$_{LZ4}$ indicates the LZ4 compression ratio for each chunk.

2. Number of Future Delta Chunks (NewDeltaNum):
\begin{equation}
    \label{eq:newDeltaNum}
    \small
    \normalsize
    NewDeltaNum = \frac{DChunkNum}{BChunkNum}
\end{equation}

3. Compression Ratio of Future Delta Chunks (DeltaRatio):
\begin{equation}
    \label{eq:deltaRatio}
    \small
    \normalsize
    DeltaRatio = \frac{1}{DChunkNum} \sum_{i=1}^{DChunkNum} Ratio_{Delta}(Chunk_i)
\end{equation}

These metrics combine to form an Estimate Ratio:
\begin{equation}
    \label{eq:estimateRatio}
    \small
    \normalsize
    EstimateRatio = LZ4Ratio + NewDeltaNum * DeltaRatio
\end{equation}

This ratio estimates the potential long-term compression efficiency if the current chunk serves as a base chunk. When $EstimateRatio$ exceeds the current chunk's compression ratio, we selectively reject the current delta compression result and either switch to the alternative resemblance detection method or designate the chunk as a new base chunk.

To fine-tune this decision-making process, we introduce a scaling factor $\alpha$ (default: $10$) to adjust the DeltaRatio's influence on the EstimateRatio. This factor helps balance between maintaining high-quality base chunks for future versions and achieving optimal immediate delta compression coverage.
```

### For Header-aggregate-chunk
```
\paragraph{For Header-Aggregate-chunk:} We recognize that Header-Aggregate-chunks cannot be deduplicated directly, making it essential to improve their overall compression ratio. We observe that many header fields (such as username, group name, and permission bits) exhibit significant repetition patterns across tar archives, suggesting that delta compression could yield substantial benefits for these chunks. However, our analysis reveals that when using traditional feature-based methods, these chunks show relatively low delta compression coverage.

Beyond the consistently identical fields mentioned above, we identify the name field as particularly promising for delta compression due to its substantial proportion (approximately 1/5 of each header). While each header's name field differs, we argue that the pathname components often contain redundant elements. Therefore, similar to our approach with File-Chunks, we select one header's name field from each Header-Aggregate-chunk, truncate up to n (default=2) trailing path components, and compute a hash value to use as a key for querying or inserting into the nameHash-index. When path depth is insufficient, we truncate fewer or no components to preserve meaningful path information, but always remove at least the filename component.

  

%%% 修改前：从每个 Header-Aggregate-chunk 中的一个 header 截断最多 n 个（默认值为 2）尾部路径组件，计算哈希值并将其记录在 nameHash-index 中。（如果路径深度不足，则会截断较少的组件或不截断任何组件，以保留有意义的路径信息。）对于传入的唯一 Header-Aggregate-chunk，FineTAR 首先尝试在元数据索引中识别匹配模式，然后在必要时采用基于特征的方法。


%%% 修改后：Header-Aggregate-chunk作为一定无法去重的chunk，提高其整体压缩率是有必要的，回想一下，我们前面提到许多标头字段（例如用户名、组名和权限位）在整个tar档案中表现出明显的重复模式，这意味着该类型的chunk做增量压缩普遍有较高收益，但是我们观察到使用传统基于特征值方法时，该类型chunk的增量压缩chunk占比较少(===========)。        抛开上述提到的一定相同的字段，有冗余部分时做增量压缩收益较高的字段我们认为是name field，这是因为name字段占据整个header约1/5的比例，尽管每个header的name field不同，但而其中pathname部分有部分冗余的可能。                            于是类似于上述对File-Chunk的处理，但这次我们从每个 Header-Aggregate-chunk 中的选择一个 header 的name field，截断最多 n 个（默认值为 2）尾部路径组件，计算哈希值作为key用于查询或插入 nameHash-index 中。（如果路径深度不足，则会截断较少的组件或不截断任何组件，以保留有意义的路径信息，但至少删掉了file name。）
```


### For File-Chunk
```
%%%原文汉语如下： For File-chunk: FineTAR 从其关联的文件头中提取路径和文件名，并计算哈希值存储在 nameHash 索引中。处理唯一的文件块时，系统首先查询此索引，以识别与先前备份中名称匹配的文件块。这种基于元数据的方法利用了以下观察结果：在备份版本中保留相同名称的文件通常具有较高的内容相似性，从而实现了高效的增量压缩，而无需进行特征提取的计算成本。如果未找到匹配的名称，系统将回退到传统的基于特征的检测方法。  

%%%想修改为如下：For File-chunk: File关联的header中的name field由版本号+路径名+文件名组成（字段合计100B），其中同一个文件对应的header的路径名+文件名的部分并不会随着文件内容修改而改变，我们可以将 路径名+文件名 这部分作为File-chunk的persistent identifier。出于减少index overhead的考虑，我们为每个将剔除版本号后的name字段取hash值并作为key，该File-Chunk为value制作索引表。    具体而言，处理unique File-Chunk时，用上述方式计算name的hash值，系统首先用该NameHash为key查询此索引表（NameHash-Index），期待匹配到修改前版本的同名File-Chunk并基于其做增量压缩；若未匹配到相似块，将继续使用传统的基于特征的检测方法来尝试匹配（因为存在一些其他可能：（1）文件修改了路径或名称 （2）初次引入的文件与系统中已经存在的文件内容很相似）；如果都未能匹配到相似块，那么该File-Chunk将作为basechunk记录到nameHash索引表和Feature索引表中。       该方法不仅较为稳定地为File-Chunk增加了 修改前版本 作为相似块选择，还有效减少了特征值的计算量（因为为name字段做hash值的时间远小于为chunk 内容算特征值的时间）。

We observe that the name field in File headers consists of version number, pathname, and filename components (totaling 100B). Crucially, the pathname and filename portions remain unchanged even when file content is modified across versions. We leverage this insight to establish a persistent identifier for File-chunks that remains stable despite content changes.

To reduce index overhead, we compute hash values of these name fields (excluding version numbers) as keys, with corresponding File-Chunks as values in our NameHash-Index. When processing unique File-Chunks, we calculate the nameHash and query this index, expecting to identify same-named chunks from previous versions for delta compression. If no match is found through metadata, we employ traditional feature-based detection to handle scenarios where (1) files have been renamed or relocated or (2) newly introduced files share content similarities with existing files. If both methods fail to identify suitable base chunks, the File-Chunk is designated as a base chunk and recorded in both nameHash and feature indices.

This technique not only provides stable base chunk selection for File-Chunks by identifying their pre-modification versions, but also significantly reduces computational overhead since calculating hash values for name fields requires substantially less processing time than generating content-based feature values.
```

这种限制的出现是因为块的标识符都完全基于chunk内容。  
用于为target chunk匹配相似块的的标识符为特征值，即使内容发生轻微变化，相应的特征值也有可能发生改变。  
我们的分析表明，用sachunking后，基于特征值的方法会有xx%-xx%的unique chunk在明确存在pre-modification的前提下未能匹配到相似块（详见第~\ref{sec:evaluation} 节），这些chunk作为base chunk做了无损压缩，损失了增量压缩机会。


待修改：
1. [x] motivation把tar archive介绍中加上少量存在的其他条目（而非只有file entry）
2. [x] Multi-Meta-chunk 改名叫 Header-Aggregate-chunk，此外要同步于ppt的图才行。此外用size字段的地方要加上向上取整
### metadata-guided2
```
我们期待于每个chunk修改时，都能为它匹配到修改前的版本作为basechunk进行增量压缩，但是这个是无法做到的。这是因为chunk唯一性的标识符为fp不能作用于相似块匹配（回想一下，只有chunk完全重复时才具有相同的fp），Feature-based方法能做到部分预期的效果，但始终有较多chunk无法匹配到修改前的版本（数据参考于ref{}）。所以我们希望有一个不随着chunk内容改变的特殊标识符来为chunk匹配到修改前版本。
于是我们提出了metadata-guided resemblance detection。我们希望通过文件名来实现这一点。我们发现，tar包中的文件名在不同版本之间是保持一致的，这就为我们提供了一个不随着chunk内容改变而一直存在的特殊标识符来实现这一点。
```



```
\subsection{Metadata-guided Resemblance Detection}
\label{subsec:metadata-guided}

We observe that traditional feature-based resemblance detection methods often fail to recognize version relationships between files that have undergone substantial modifications. Through our experimental analysis across multiple datasets, we identify that these pre-modification version mismatches account for 28.5-63.7\% of all potential matching scenarios (see Table~\ref{tab:matching-cases} in \S\ref{sec:evaluation}). These missed opportunities occur when feature-based detection fails to match files with their previous versions, despite clear semantic relationships between them.

When employing feature-based methods for file-level delta compression, we encounter two primary matching scenarios (Fig.~\ref{fig:deltamotivation}): (i) \textbf{File creation from templates}, where newly created files match their template files, allowing storage of only differences between the new file and the template; and (ii) \textbf{File modification}, where a file's most suitable base would be its pre-modification version. However, we find that feature-based methods often fail in the second scenario, as even moderate modifications can prevent successful matching when relying solely on content similarity with high thresholds.

\begin{figure}[t]
    \centering
    \includegraphics[width=3.4in]{pic/design/Metadata-guided.png}
    \vspace{-8pt}
    \caption{Metadata-guided Resemblance Detection architecture: Prioritizing efficient metadata matching with feature-based detection as secondary approach.}
    \vspace{-12pt}
    \label{fig:Metadata-guided}
\end{figure}

\subsubsection{Dual-modal Resemblance Detection Approach}

We address these limitations by proposing a dual-modal resemblance detection system in FineTAR that combines metadata guidance with conventional feature-based methods. We argue that filename consistency across backup versions provides a reliable indicator of file relationships, regardless of content changes. This approach specifically targets the "pre-modification version mismatch" problem, enabling us to identify version relationships when content features fail to recognize heavily modified files.

Benefiting from our semantic-aware chunking that effectively extracts and preserves metadata, FineTAR leverages the rich metadata information naturally present in tar archives to enable rapid identification of potential similar chunks without the computational overhead of content analysis. We identify two key advantages of this approach: (1) metadata-based matching can detect similarities even when content modifications invalidate traditional feature values, and (2) it requires significantly fewer computational resources than generating and comparing content hashes.

\subsubsection{Implementation for Different Chunk Types}

We implement metadata-guided detection differently across chunk types based on their characteristics:

\begin{itemize}
\item \textbf{For File-chunk:} We extract path and filenames from associated headers and compute hash values for the nameHash-index. When processing a unique File-chunk, we first query this index to identify chunks with matching names from previous backups. This approach leverages our observation that files retaining identical names across backup versions typically exhibit high content similarity, enabling efficient delta compression without feature extraction costs. We fall back to traditional feature-based detection only when no matching names are found.

\item \textbf{For Multi-Meta-chunk:} We truncate up to n (default=2) trailing path components from one header within each Multi-Meta-chunk, compute a hash value, and record it in the nameHash-index. For incoming unique Multi-Meta-chunks, we first attempt to identify matching patterns in the metadata index before resorting to feature-based methods.

\item \textbf{For CDC-chunks:} We recognize that large files produce multiple CDC-chunks, making one-to-one filename mapping impractical. Therefore, we apply feature-based detection directly to identify similar chunks based on content characteristics.
\end{itemize}

\subsubsection{Balancing Metadata-guided and Feature-based Approaches}

We combine metadata-guided and feature-based approaches as complementary methods with distinct advantages. We find that metadata-guided detection effectively identifies similarities despite content modifications, particularly for files maintaining their names despite changes. Additionally, it significantly reduces computational overhead by eliminating redundant feature calculations when metadata matching succeeds.

Conversely, we leverage feature-based detection for identifying similar chunks regardless of names, which proves valuable when processing files with different names but similar content. Given these complementary strengths, we employ a sequential strategy: first attempting metadata-guided detection for efficiency and lower computational cost, then falling back to feature-based detection when necessary.

We address the potential issue of suboptimal results from metadata matching through a false detection filter (Section~\ref{subsec:falsefilter}). We recognize that metadata similarity does not guarantee content similarity, so our filter evaluates delta-compressed chunk quality and dynamically adjusts the detection strategy based on historical performance, ensuring optimal compression efficiency while maintaining high throughput across diverse backup workloads.
```

### metadata-guided1
```
\subsection{Metadata-guided Resemblance Detection}

\label{subsec:metadata-guided}

  

%%%% (1) pre-modification version mismatch -->

  
  

We identify that traditional feature-based methods frequently fail to recognize version relationships when files undergo substantial modifications, resulting in significant compression opportunities being missed. To address these limitations, we propose a dual-modal resemblance detection system in FineTAR that combines metadata guidance with conventional feature-based methods. We argue that filename consistency across backup versions provides a reliable indicator of file relationships, regardless of content changes. This approach particularly addresses the "pre-modification version mismatch" problem discussed earlier, where we can leverage metadata to identify version relationships when content features fail to recognize heavily modified files.

  

\begin{figure}[t]

    \centering

    % \includegraphics[width=3.4in]{pic/sachunking.pdf}

    \includegraphics[width=3.4in]{pic/design/Metadata-guided.png}

    \vspace{-8pt}

    \caption{Metadata-guided Resemblance Detection architecture: Prioritizing efficient metadata matching with feature-based detection as secondary approach.}

    \vspace{-12pt}

    \label{fig:Metadata-guided}

\end{figure}

  
  

Benefiting from the semantic-aware chunking that effectively extracts and preserves metadata, FineTAR introduces a metadata-guided resemblance detection method to enhance delta compression efficiency. By leveraging the rich metadata information naturally present in tar archives, this innovative approach enables rapid identification of potential similar chunks without the computational overhead of content analysis. The key advantage is that metadata-based matching can detect similarities even when content modifications would invalidate traditional feature values, while requiring significantly less computational resources than generating and comparing content hashes.

  

To handle cases where metadata matching alone is insufficient, FineTAR incorporates a complementary feature-based detection method. Different chunk types utilize metadata-guided detection in varying ways based on their characteristics, with feature-based detection serving as a fallback mechanism when necessary. Below, we detail how each chunk type leverages metadata information and when the system switches to traditional feature-based approaches:

\begin{itemize}

\item \textbf{For File-chunk:} FineTAR extracts path and filenames from their associated headers and computes hash values for storage in the nameHash-index. When processing a unique File-chunk, the system first queries this index to identify chunks with matching names from previous backups. This metadata-based approach leverages the observation that files retaining identical names across backup versions typically exhibit high content similarity, enabling efficient delta compression without the computational cost of feature extraction. If no matching names are found, the system falls back to traditional feature-based detection.

  

\item \textbf{For Multi-Meta-chunk:}  The system truncates up to n (default=2) trailing path components from one header within each Multi-Meta-chunk, computes a hash value, and records it in the nameHash-index. (If the path depth is insufficient, fewer or no components are truncated to maintain meaningful path information.) For incoming unique Multi-Meta-chunks, FineTAR first attempts to identify matching patterns in the metadata index before resorting to feature-based methods when necessary.

  

\item \textbf{For CDC-chunks:} Since large files produce multiple CDC-chunks, maintaining one-to-one filename mapping becomes impractical. Therefore, FineTAR directly applies feature-based detection to identify similar chunks based on content characteristics.

\end{itemize}

  

In resemblance detection, we combine metadata-guided and feature-based approaches, as each method has its distinct advantages. Metadata-guided detection can effectively identify similarities even when content modifications would invalidate traditional feature values, which is particularly beneficial for files that maintain their names despite undergoing changes. Additionally, it significantly reduces computational overhead by eliminating redundant feature calculations when metadata matching succeeds.

  

Conversely, feature-based detection excels at identifying similar chunks regardless of their names, which is particularly valuable when processing backup data containing files with different names but similar content. Given these complementary strengths, FineTAR employs a sequential strategy: first attempting metadata-guided detection for its efficiency and lower computational cost, then falling back to feature-based detection when necessary to ensure comprehensive coverage of similarity detection.

  

To address cases where metadata matching produces suboptimal results (as metadata similarity does not guarantee content similarity), FineTAR incorporates a false detection filter (Section~\ref{subsec:falsefilter}) that evaluates the quality of delta-compressed chunks. This mechanism dynamically adjusts the detection strategy based on historical performance, ensuring optimal compression efficiency while maintaining high throughput across diverse backup workloads.
```

### sachunking3
```
%%  when we apply cdc in the file regions and header regions, we'll get more small chunks, which will increase the metadata overhead.

Having established file boundaries (as illustrated in Figure~\ref{fig:sachunking}), we obtain numerous chunks of various types. However, directly using these boundaries as final chunk delineations presents two significant challenges: Firstly, headers at only 512B are excessively small, substantially increasing the metadata overhead for the backup system. Secondly, applying CDC across all file regions would further reduce average chunk size, exacerbating metadata management overhead - this motivates our decision to treat smaller files as single chunks. Meanwhile, excessively large files treated as individual chunks may compromise system performance. To address these challenges, we reorganize the data blocks into three distinct chunk types while preserving file boundaries, based on size field and type field:

```


sachunking2
```
%%%% size

We propose a semantics-aware chunking approach that aligns chunk boundaries with file boundaries by leveraging the inherent structure of tar archives. In tar format, files and their metadata are organized in a well-defined structure where file boundaries can be precisely identified by parsing specific fields in the headers and their encapsulated metadata.

Each file is preceded by a file header block (header that manages file metadata; we refer to these as "file headers" to distinguish them from other header types in the tar format). By parsing the {\em size} field in these file headers (as shown in Figure~\ref{fig:sachunking}), we can determine exact file boundaries. For example, using the file header's beginning as a reference point {\em H}, the file header's boundary would be defined as {\em (H, H+512)}, the file's boundary would be defined as {\em (H+512, H+512+size)}.

  

%%%%type

Additionally, tar archives primarily consist of 512B file header blocks and variable-length files, but also include special 512B metadata header blocks that contain only metadata (such as directories, symbolic links, and device files) which, unlike file headers, are not followed by file content.

Without distinguishing between file header and metadata header, we would extract incorrect file boundaries (metadata header's {\em size} field is 0), potentially causing out-of-bounds errors. However, by utilizing the {\em type} field, we effectively resolve this issue (we only use the boundary judgment scheme mentioned above when we determine that the file header is a file header. If it is a metadata header, we only process the boundary of the current metadata header as {\em (H, H+512)}, and then parse the size and type of the next header).

  

Having established file boundaries (as illustrated in Figure~\ref{fig:sachunking}), we obtain numerous chunks of various types (File blocks, file header blocks, metadata header blocks). However, directly using these boundaries as final chunk delineations presents two significant challenges: Firstly, headers and other metadata chunks at only 512B are excessively small, substantially increasing the metadata overhead for the backup system. Secondly, excessively large files treated as single chunks may compromise system performance and delta compression efficiency. To address these challenges, we reorganize the data blocks into three distinct chunk types while preserving file boundaries, based on size and type:

\paragraph{Multi-Meta-chunk}: For file header blocks and metadata header blocks, which uniformly occupy 512B each, processing individual headers as separate chunks would impose substantial metadata overhead on the system. Our insight is to aggregate multiple headers (default: 16 headers, aligning with the common 8KiB average chunk size in backup systems) into a single Multi-Meta-chunk for both deduplication processing (lossless or delta compression) and metadata management.

  

In conventional scenarios, merging multiple small chunks into a single chunk typically represents a tradeoff between storage efficiency and metadata overhead. This occurs because: (1) content changes in any constituent small chunk prevent deduplication of the entire aggregated chunk, including unchanged components, and (2) insertion of a small chunk at the beginning shifts all subsequent content, preventing deduplication of the entire Multi-chunk. Generally, as n (number of aggregated chunks) increases, deduplication efficiency decreases.

  

However, our insight is that this pattern does not apply to header blocks. We observe that the modification time field in headers (tar archives treat packaging time as modification time) invariably changes between backup versions, and consequently, the checksum calculated from header content also changes. This fundamentally eliminates any possibility of deduplication for chunks containing headers.

  

Conversely, aggregating adjacent headers into Multi-Meta-chunks offers storage advantages for several reasons. Headers in tar archives are ordered according to the name field (pathname + filename), meaning that when adjacent headers are aggregated into a single chunk for lossless compression, the pathname components have a high probability of being deduplicated. Additionally, username, group name, and modification time fields have significant repetition probability regardless of adjacency. In other words, aggregating multiple headers into Multi-Meta-chunks achieves a win-win design between metadata overhead reduction and storage efficiency, a finding validated by the results presented in Figure~\ref{fig:MultHeader}.

  

\paragraph{File-chunk and CDC-chunks}: Our key insight is to preserve file boundaries while consolidating data from the same file into a single chunk whenever possible, which benefits both deduplication and delta compression redundancy elimination (see challenges in \S\ref{subsec:motivation}). However, excessively large chunks can significantly burden delta compression performance, as delta compression essentially builds a dictionary of character combinations from the base chunk to match against the target chunk. When chunk size grows too large, the dictionary becomes unwieldy, increasing matching overhead. Therefore, we establish a threshold to differentiate between small and large files, enabling special handling for larger files. We set this threshold at 4 MiB, which aligns with the container size commonly accepted in backup systems \cite{zhu08}, ensuring our designated "small files" never exceed backup system container capacities.

  

For files smaller than 4 MiB, we process the entire file as a single {\em File-chunk}. This approach perfectly aligns chunk boundaries with file boundaries, which directly fulfills our primary goal of eliminating boundary shifts. File-chunks provide several advantages: (1) simplified metadata management with one-to-one file-to-chunk mapping, increasing average chunk size and reducing metadata overhead; (2) reduced processing overhead by avoiding complex chunking algorithms (particularly the computation-intensive rolling hash calculation); and (3) optimized handling of small file modifications, as changes remain contained within a single chunk. Due to the alignment with file boundaries, delta compression can store only the actual changes to the file, eliminating redundant storage of unchanged content that would otherwise result from boundary shifts.

  

For files larger than 4 MiB, we implement FastCDC to generate {\em CDC-chunks} while preserving file boundaries. This does not mean our approach for large files is equivalent to pure FastCDC implementation, as we first ensure both starting and ending boundaries of the entire large file, preventing cross-file cascading boundary shifts. Processing larger files as CDC-chunks offers distinct benefits: (1) enabling partial file deduplication even when modifications occur elsewhere in the file; (2) maintaining manageable chunk sizes that optimize delta compression algorithm efficiency.
```



sachunking1
```
% To ensure file boundaries align with chunk boundaries, we propose a Semantics-aware Chunking method that utilizes tar header metadata to determine chunk boundaries. The process begins by sequentially reading and analyzing tar file headers. Following the UNIX POSIX Tar specification, we extract metadata fields (name, size, and type, As shown in the Header block section of Figure~\ref{fig:sachunking}) based on header offset positions. Our method calculates the ending offsets of both header and data blocks to establish file semantic boundaries, creating chunks that preserve header context while separating headers from their associated data blocks.

  

% For the following data blocks, we generate three kinds of chunks based on their file size and type as follows:

  

% \begin{itemize}

%     \item \textbf{File-chunks}:  For files smaller than 4 MiB, we aggregate their data blocks into a single File-chunk. This strategy ensures efficient deduplication of unchanged files and captures modifications into one chunk, optimizing metadata-guided delta compression.

%     \item \textbf{Multi-Meta-chunks}: For special file entries (directories, symbolic links, and device files) containing only metadata, we generate Multi-Meta-chunks from their headers, preserving essential information while minimizing storage overhead.

%   \item \textbf{CDC-chunks} (Content-Defined chunks): For files larger than 4 MiB, we implement FastCDC to generate CDC-chunks. This approach maintains uniform chunk sizes within a specified range, enhancing delta compression efficiency by preventing chunk size misalignment.

% \end{itemize}

  
  

% Figure \ref{fig:sachunking} provides a clear visual representation of our Semantic-aware Chunking approach. This innovative method addresses the boundary instability problem in tar backup systems by utilizing file semantic information rather than content patterns to determine chunk boundaries.

  

% As illustrated in the figure, our approach fundamentally differs from traditional content-defined chunking in handling file modifications. When a file changes (File 2 in the illustration), traditional CDC methods (upper portion) trigger a cascade of boundary shifts that propagate to subsequent unmodified files (File 3), significantly reducing deduplication efficiency across backup versions. In contrast, our Semantic-aware Chunking (lower portion) maintains stable file boundaries by anchoring chunk delineation to file semantic information extracted from tar headers.

  

% The figure effectively demonstrates how our method processes the tar archive stream by precisely identifying file boundaries through metadata analysis. By leveraging header information to determine chunk boundaries, we create a more stable chunking foundation that preserves semantic file relationships while preventing cross-file boundary shifts. This approach directly addresses the 36.0\% shift chunk ratio problem observed in our motivation study, ensuring that modifications to one file remain isolated and do not affect the chunking of other unchanged files.

  

% This semantic preservation is particularly valuable for tar backups where logical file boundaries carry significant meaning but are often ignored by traditional content-based approaches. By aligning chunking boundaries with file semantics, we maximize deduplication opportunities while maintaining system performance through efficient metadata processing.

  

% This semantic-aware approach directly addresses the boundary unstability problem observed in our motivation study by ensuring that file boundaries are preserved, preventing the propagation of boundary shifts between files.

  

%% pseudocode & its description

  

% \begin{algorithm}[!ht]

%   \caption{SA Chunking}

%   \begin{algorithmic}[1]

%       \Function{SAChunking}{$src, end$}

%       \State $Type \gets HEADER$

%       \While {$src < end$}

%       \If {$Type = HEADER$}

%       \State $Type \gets src[156]$

%       \State $size \gets 0$

%       \For {$i = 0$ to $10$}

%       \State $size \gets size \times 8 + src[124 + i]$

%       \EndFor

%       \State $size \gets \left\lceil \frac{size}{512} \right\rceil \times 512$

  

%       \If {$Type = FILE$ and $size \geq 4MiB$}

%       \State $Type \gets BIGFILE$

%       \EndIf

%       \State Append to header buffer

%       \State when the buffer is full, pushes the buffer

%       \State $src \gets src+512$

%       \ElsIf {$Type = FILE$}

%       %\State Allocate memory for file chunk

%       \State $Type \gets HEADER$

%       \State $src \gets src+size$

  

%       \ElsIf {$Type = BIGFILE$}

%       \State $cpSum \gets 0$

%       \While {$cpSum<size$}

%       \State $cpSum += FastCDC(src+cpSum)$

%       %\State Allocate memory for CDC chunk

%       \EndWhile

%       \State $Type \gets HEADER$

%       \State $src \gets src+size$

%       \Else

%       \State Append to header buffer

%       \State when the buffer is full, pushes the buffer

%       \State $Type \gets HEADER$

%       \State $src \gets src+512$

%       \EndIf

%       \EndWhile

%       \EndFunction

%   \end{algorithmic}

%   \label{SA-algorithm}

% \end{algorithm}

  

% Figure \ref{fig:sachunking} illustrates our Semantics-aware Chunking process. The algorithm takes the source data ($src$) and its end position ($end$) as input. For each iteration, it first processes a header block by extracting the file type and size information from specific offset positions (lines 4-11). Based on the file type and size, it categorizes the data into different chunk types: regular files (FILE), large files (BIGFILE), or metadata-only entries. For header blocks, the algorithm accumulates them in a buffer until reaching 8KiB (16 blocks) before generating a Multi-Meta-chunk (lines 13-15). For regular files smaller than 4MiB, it processes the entire file as a single File-chunk (lines 16-18). For files larger than 4MiB, it applies FastCDC to generate variable-sized chunks within the file's data region (lines 19-24). For special file types containing only metadata, it processes them similarly to header blocks (lines 25-29).
```