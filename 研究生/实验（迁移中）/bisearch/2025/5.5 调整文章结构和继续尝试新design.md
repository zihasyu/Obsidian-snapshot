
## 上周提到的问题

- design1比较straight forward。
![[sachunking2 1.png]]

- design2**过分**straight forward，尤其是对大文件的处理（CDC-chunks）。
![[Metadata-guided 1.png]]

- design1解决motivation提出的问题后，并没有通过引出问题来引出design2，而是一个方法引出方法，这不利于paper整体性。
## 讨论的结果

### 调整文章结构

 - **将sachunking和meta-guided detection （上面有这两个design的简要图）合并为 design1 基于元数据的极致化去重工作流。**

	我们利用header里的元数据字段(name,size,type等等)对齐了文件边界与chunk边界，解决了motivation里提出的边界偏移问题。并从chunking，去重到压缩提出一整套去重工作流。
	
	有关delta compression的部分是：基于name去做滚动哈希值（nameHash），用这个哈希值去匹配相似块，能为delta compression提供额外的base chunk选项，且通过这种方式比基于chunk content做滚动哈希值提取特征值的performance更好。
	
	这样写，会使得design1很丰富，不再有sachunking和meta-guided detection过于straight forward，及没有因问题引出下一个design的这两个上周提到的问题。
	
	（至于如何用新design1导致的问题引出下一个design 见下文）。


 - **原本的design3（下图）提到design2，同时可以做到由design1的问题引出design2**
	 design1的极致化去重工作流存在以下问题：当file-chunk变化过大时，我们的metadata-guided detection(图里简化为了metadata detection) 仍然会找到对应的file-chunk 作为base chunk。
	 
	 只考虑当前这个backup的存储，这样无疑是符合短期收益最优的。但考虑到长期收益，我们需要将一些变化过大的file-chunk拒绝掉，以为未来提供更优质的base chunk。
	
![[FalseFilter.png]]

- **尝试紧密围绕Tar特性的新design作为design3**
（see below）




### 尝试新design

#### 出发点：

如果每个版本有大量file是完全重复的，那么随着backups数量的增加，Headers对存储系统的影响是越来越大的（虽然logical上占比很小，但去重后占比越来越大，**下面有先行实验证明**）。这是因为Tar里面的Header无法重复，file却存在重复的可能性。

所以当用户能接受元数据有损压缩时，可以只保留Header中必要的部分，舍弃其余字段，进一步提高saving。（做消融实验，讲明什么时候特别有效，让用户自己选择是否使用有损压缩，还是仅使用前面design以作为无损压缩）。

如果这个design做出来是有效的，那么确实是和Tar特性很紧密的一个design。
#### 先行实验

用我们当前的方法做整个流程，并记录Headers的logical size和save size(去重后或压缩后的)，然后分别计算Headers在**logical**和**去重&压缩**后分别的占比。发现虽然logical上Headers确实占比很小，但一些数据集上**去重&压缩**后占比会更显著。特别是chromium，Headers对最后系统存储占比到了49%。这意味着进一步优化Header是有意义的。

- **Save Size**指整个去重工作流（去重+压缩）后的剩余大小
- Header Chunk Logical Size / Overall Logical Chunk Size 。Header logical size占总数据集logical size的占比。
- Header Chunk Save Size / Overall Save Chunk Size。Header Save Size 占总体Save Size的占比

| Dataset        | Backups | Header Chunk Logical Size / Overall Logical Chunk Size | Header Chunk Save Size / Overall Save Chunk Size |
| -------------- | ------- | ------------------------------------------------------ | ------------------------------------------------ |
| ==linux==      | ==270== | ==7777042432/212796651520 ≈ 3.66%==                    | ==492982820/2531759177 ≈ 19.47%==                |
| ==WEB==        | ==102== | ==2989658624/297546045440 ≈ 1.00%==                    | ==214631582/1209744694 ≈ 17.74%==                |
| ==chromium==   | ==107== | ==23449270784/409560760320 ≈ 5.73%==                   | ==1587123456/3188542670 ≈ 49.79%==               |
| ==automake==   | ==100== | ==50197504/504084480 ≈ 9.96%==                         | ==3077260/21184428 ≈ 14.53%==                    |
| bash           | 44      | 31733248/1374453760 ≈ 2.31%                            | 1772324/131901186 ≈ 1.34%                        |
| coreutils      | 28      | 22265856/240936960 ≈ 9.24%                             | 1297809/13145498 ≈ 9.87%                         |
| fdisk          | 22      | 1180672/16619520 ≈ 7.10%                               | 59546/1431627 ≈ 4.16%                            |
| ==glibc==      | ==100== | ==718227456/14690037760 ≈ 4.89%==                      | ==44280955/271694053 ≈ 16.30%==                  |
| ==GCC==        | ==117== | ==3551665664/44142602240 ≈ 8.05%==                     | ==219356867/1239167953 ≈ 17.70%==                |
| ThunderbirdTar | 240     | 14150144/31809034240 ≈ 0.04%                           | 2416511/3665676718 ≈ 0.07%                       |
| Android        | 36      | 25888256/3655925760 ≈ 0.71%                            | 2407847/354540760 ≈ 0.68%                        |
- 这是FineTAR方法得出的数据（sachunking那里把header聚在一起，其实这里已经意识到了**Header存在瓶颈**，所以做了Header的优化，这是利于lossless compression的。但即便做了这个优化，在一些数据集上Header压缩后还是占了较多空间）
#### 新design有损压缩的实现以及实验

##### 第一次尝试

| 数据集            | 参数       | Overall Compression Ratio | Header OCR | File OCR |
| -------------- | -------- | ------------------------- | ---------- | -------- |
| Android        | -c 4     | 10.3117                   | 10.7516    | 10.3087  |
| Android        | -c 8     | 10.3262                   | 14.5263    | 10.305   |
| automake       | -c 4     | 23.795                    | 16.3124    | 25.0667  |
| automake       | -c 8     | 24.5592                   | 21.5079    | 24.9506  |
| bash           | -c 4     | 10.4203                   | 17.9049    | 10.3184  |
| bash           | -c 8     | 10.4012                   | 22.3885    | 10.2712  |
| coreutils      | -c 4     | 18.3285                   | 17.1565    | 18.4569  |
| coreutils      | -c 8     | 18.6423                   | 21.1381    | 18.4208  |
| fdisk          | -c 4     | 11.6088                   | 19.8279    | 11.2521  |
| fdisk          | -c 8     | 11.7044                   | 24.3302    | 11.2577  |
| glibc          | -c 4     | 54.0683                   | 16.2198    | 61.438   |
| glibc          | -c 8     | 55.5401                   | 21.0849    | 60.6334  |
| smalltalk      | -c 4     | 34.1479                   | 16.6266    | 35.8433  |
| smalltalk      | -c 8     | 34.648                    | 22.6863    | 35.4878  |
| GCC            | -c 4     | 35.6228                   | 16.1913    | 39.8024  |
| GCC            | -c 8     | 36.6825                   | 20.1516    | 39.5191  |
| ThunderbirdTar | -c 4     | 8.67753                   | 5.85561    | 8.6794   |
| ThunderbirdTar | -c 8     | 8.67833                   | 6.7216     | 8.67946  |
| ==chromium==   | ==-c 4== | ==128.448==               | 14.7747    | 241.106  |
| ==chromium==   | ==-c 8== | ==157.964==               | 23.6576    | 241.085  |
| linux          | -c 4     | 84.0509                   | 15.7755    | 100.56   |
| linux          | -c 8     | 86.6436                   | 19.7547    | 99.4121  |
| ==WEB==        | ==-c 4== | ==245.958==               | 13.9293    | 296.003  |
| ==WEB==        | ==-c 8== | ==257.568==               | 19.0766    | 295      |

##### 尝试进一步有损
发现header的name字段会存储版本号，尝试将版本号删掉

| 数据集            | FineTAR Overall OCR | Lossy Overall OCR     | FineTAR Header OCR | Lossy Header OCR       | FineTAR File OCR | Lossy File OCR   |
| -------------- | ------------------- | --------------------- | ------------------ | ---------------------- | ---------------- | ---------------- |
| Android        | 10.3117             | 10.3262 (+0.14%)      | 10.7516            | 14.7689 (+37.36%)      | 10.3087          | 10.3041 (-0.04%) |
| ==automake==   | ==23.795==          | ==25.7243 (+8.11%)==  | ==16.3124==        | ==34.3413 (+110.52%)== | 25.0667          | 25.0297 (-0.15%) |
| bash           | 10.4203             | 10.2428 (-1.70%)      | 17.9049            | 27.8722 (+55.67%)      | 10.3184          | 10.092 (-2.19%)  |
| coreutils      | 18.3285             | 18.7841 (+2.49%)      | 17.1565            | 23.2307 (+35.40%)      | 18.4569          | 18.425 (-0.17%)  |
| fdisk          | 11.6088             | 11.7354 (+1.09%)      | 19.8279            | 26.3666 (+32.98%)      | 11.2521          | 11.2577 (+0.05%) |
| ==glibc==      | ==54.0683==         | ==58.3803 (+8.01%)==  | ==16.2198==        | ==34.3234 (+111.61%)== | 61.438           | 60.5623 (-1.43%) |
| smalltalk      | 34.1479             | 35.126 (+2.86%)       | 16.6266            | 29.3877 (+76.75%)      | 35.8433          | 35.4366 (-1.13%) |
| ==GCC==        | ==35.6228==         | ==37.9304 (+6.48%)==  | ==16.1913==        | ==25.921 (+60.09%)==   | 39.8024          | 39.5331 (-0.68%) |
| ThunderbirdTar | 8.67753             | 8.67907 (+0.02%)      | 5.85561            | 9.69838 (+65.62%)      | 8.6794           | 8.67866 (-0.01%) |
| ==chromium==   | ==128.448==         | ==187.395 (+45.89%)== | ==14.7747==        | ==40.1686 (+171.87%)== | 241.106          | 241.051 (-0.02%) |
| ==linux==      | ==84.0509==         | ==92.2649 (+9.77%)==  | ==15.7755==        | ==35.8582 (+127.30%)== | 100.56           | 98.1198 (-2.43%) |
| ==WEB==        | ==245.958==         | ==256.953 (+4.47%)==  | ==13.9293==        | ==18.7421 (+34.55%)==  | 296.003          | 295.01 (-0.34%)  |


## 讨论提到的废案以及原因

- 将meta-guided和falsefilter合起来讲delta compression，以解决design2过于straight forward的原因
	1. 这还是没能解决sachunking与后续割裂的问题
	2. 难以讲清，falsefilter的动机是长期收益，需要先讲meta-guided才能暴露出来的。
	3. sachunking到delta compression design的过渡仍是从方法到方法，不是问题到方法。

- 给大文件做一个额外的处理，使design2不那么straight forward
	1. motivation提出的问题是 跨文件的级联偏移影响去重和增量压缩，这意味着我们没能给大文件内部找到问题，所以本质上没有给这个部分做额外处理的动机（不如仍保持sota）。
	2. 对大文件做过一些尝试，效果不如保持原先的做法。
	3. 即便解决了这个问题，也没能处理好如何从design1引出的问题来引出design2，仍是方法引出方法的“散”的文章。