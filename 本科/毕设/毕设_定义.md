### 神经网络

##### 定义：
神经网络又称人工神经网络 (ANN) 或模拟神经网络 (SNN)，是机器学习的子集，同时也是深度学习算法的核心。  神经网络其名称和结构均受到人脑的启发，可模仿生物神经元相互传递信号的方式。
	人工神经网络 (ANN) 由节点层组成，包含一个输入层、一个或多个隐藏层和一个输出层。 每个节点也称为一个人工神经元，它们连接到另一个节点，具有相关的权重和阈值。 如果任何单个节点的输出高于指定的阈值，那么会激活该节点，并将数据发送到网络的下一层。 否则，不会将数据传递到网络的下一层。
	神经网络依靠训练数据来学习，并随时间推移提高自身准确性。 而一旦这些学习算法经过了调优，提高了准确性，它们就会成为计算机科学和人工智能领域的强大工具，使我们能够快速对数据进行分类和聚类。
##### 发展：
- 1943 年：Warren S. McCulloch 和 Walter Pitts 发表了“A logical calculus of the ideas immanent in nervous activity”，本研究旨在了解人脑如何通过互相连接的脑细胞或神经元形成复杂模式。 本文中的主要想法之一是将具有二进制阈值的神经元与布尔逻辑（即，0/1 或 true/false 语句）进行对比。   
- 1958 年：Frank Rosenblatt 开发出感知器，这一创新记录在他的以下研究论文中：“The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain”。 他通过在方程中引入权重，进一步巩固了 McCulloch 和 Pitt 的研究成果。 Rosenblatt 能够利用 IBM 704，让计算机学会如何区分左侧标记的卡片和右侧标记的卡片。
- 1974 年：虽然许多研究人员都对反向传播的概念做出了贡献，但 Paul Werbos 作为美国第一人，首次在其博士论文中提到了反向传播在神经网络中的应用。
- 1986年神经网络之父辛顿提出了适用于多层感知机MLP的BP算法，并在层与层之间的传播过程中引入了 Sigmoid函数，为神经网络引入了非线性，改良了单层感知机无法解决异或问题的缺陷。所谓BP算法包括两部分，即前向传播的计算与反向传播的计算。如果是权重已经训练好的神经网络模型，仅使用前向传播即可预测出结果。可是神经网络的训练还需要反向传播的参与。当前向传播预测值与真实值有出入的时候，利用梯度下降的方法，在反向传播中使模型权重改变，让模型权重更加贴合真实值。常见的梯度更新优化器有如下四种，它们分别是：SGD、Momentum、AdaGrad、Adam。
	BP模型的应用很广泛，至今仍然应用在很多流行的网络中。它为神经网络的发展发挥了重要作用，理论更加完备严谨，但是它也有其缺点。1991年BP算法被指出存在梯度消失（同样也存在梯度爆炸）问题。梯度消失（对应的是梯度爆炸）是指神经网络在反向传播时，深层次的梯度向前传播时越乘越小，最终弥散过小致使浅层的参数无法进行有效的学习。根据资料得知，若Sigmoid函数为f(x),Sigmoid函数的导数为f(x)(1-f(x)),Sigmoid函数的值域为[0,1],所以其导数值域为[0,0.25],所以Sigmoid激活函数值域范围较小的特性放大了神经网络梯度消失的问题。
- 在2006-2012年之间，对深度学习的研究逐渐增多，方法思路也日新月异，其中2011年提出的ReLU激活函数，很好得解决了Sigmoid函数在梯度传播过程中的梯度消失问题，该激活函数至今应用中各流行网络中。在2012年，辛顿课题组参加了ImageNet图像识别比赛，凭借AlexNet取得第一名，并大幅领先第二名。AlexNet在LeNet-5的基础上进行了改进，首次采用ReLU激活函数，并增加了网络的层数，使得网络可以表征更加丰富的特征。


##### 基本技术：多层感知机，后向传播算法，随机梯度下降,损失函数,随机梯度下降
MLP是神经网络的基本结构，反向传播算法用于计算梯度以便调整网络参数，而SGD是一种优化算法，用于在参数空间中搜索损失函数的最小值。这三者通常结合使用，构成了神经网络模型的训练过程。
- **多层感知器 (MLP)**：它们由输入层、一个或多个隐藏层以及输出层组成。 虽然这些神经网络通常也被称为 MLP，但值得注意的是，神经网络它们实际上是由 sigmoid 神经元而不是感知器组成的，因为大多数现实问题都是非线性的。 通常将数据馈送到这些模型进行训练，这些数据便是计算机视觉处理、自然语言处理和其他神经网络的基础。
- **激活函数**是用来加入非线性因素的，解决线性模型所不能解决的问题。
- **反向传播（back propagation, BP**）算法是 "误差反向传播" 的简称，也称为backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。反向传播是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，反向传播仅指用于计算梯度的方法。
- **损失函数**（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。作为对比，代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。
- **随机梯度下降**(Stochastic Gradient Descent，SGD)是一种优化算法，通常用于机器学习和深度学习中的模型训练过程中，特别是在大规模数据集上。其主要作用是最小化或最大化一个损失函数，使其达到最优值或接近最优值
### 强化学习
##### 定义：
**强化学习（Reinforcement Learning，RL）**\是一种机器学习方法，旨在让智能体(agent)通过与环境的交互来学习如何做出决策，以使得其在未来能够获得最大的累积奖励。
在强化学习中，智能体采取某个动作作为响应环境的当前状态，并由环境返回一个奖励信号，表示对该动作的评价。智能体的目标是通过尝试不同的动作来最大化长期累积奖励。
##### 强化学习的关键要素包括：奖励 环境 智能体 状态价值 状态动作对价值
1. **奖励（Reward）**：奖励是智能体在与环境交互时获得的反馈信号，用于评估智能体采取的动作的好坏。奖励可以是正数、负数或零，代表了动作对智能体的积极影响、消极影响或中立影响。智能体的目标是通过选择动作来最大化长期累积奖励。
2. **环境（Environment）**：环境是智能体所处的外部世界，它可以是真实世界中的一部分，也可以是模拟环境或游戏环境。环境的状态会随着智能体的动作而改变，并且环境会返回奖励信号作为对智能体动作的反馈。
3. **智能体（Agent）**：智能体是在环境中执行动作的实体，它通过观察环境的状态并选择相应的动作来实现其目标。智能体通常具有一个决策策略，用于确定在给定状态下应该采取哪个动作。
4.  **状态价值（State Value）**：状态价值指的是在某个状态下，智能体可以获得的长期累积奖励的期望值。换句话说，它衡量了智能体处于某个状态时所能达到的价值大小。状态价值可以用符号 V(s) 表示，其中 s 是状态。状态价值函数 V(s) 定义为智能体在状态 s 下的长期累积奖励的期望值。状态价值函数用于评估不同状态的好坏程度，以帮助智能体做出决策。
5. **状态-动作对价值（State-Action Value）**：状态-动作对价值指的是在某个状态下，采取某个动作后，智能体可以获得的长期累积奖励的期望值。换句话说，它衡量了在某个状态下采取某个动作的价值大小。状态-动作对价值可以用符号 Q(s,a) 表示，其中 s 是状态，a 是动作。状态-动作对价值函数 Q(s,a) 定义为智能体在状态 s 下采取动作 a 后的长期累积奖励的期望值。状态-动作对价值函数用于评估在给定状态下采取不同动作的优劣，以帮助智能体选择最佳的动作。
	状态价值衡量的是在某个状态下的长期累积奖励的期望值，而状态-动作对价值衡量的是在某个状态下采取某个动作后的长期累积奖励的期望值。状态价值函数和状态-动作对价值函数都是强化学习中的重要工具，用于帮助智能体评估和选择动作，以达到最大化长期累积奖励的目标。
##### 两种方法：蒙特卡洛方法，表格型方法
1. **蒙特卡洛方法**：
    蒙特卡洛方法是一种基于经验的学习方法，其基本思想是通过与环境的交互来收集样本轨迹，然后利用这些轨迹来估计状态值函数或状态-动作对值函数。
    在蒙特卡洛方法中，智能体不需要事先了解环境的动态规则，只需与环境进行交互，收集奖励信号，并利用这些奖励信号来估计值函数。
    典型的蒙特卡洛方法包括蒙特卡洛预测和蒙特卡洛控制。蒙特卡洛预测用于估计状态值函数或状态-动作对值函数，而蒙特卡洛控制用于找到最优策略。
2. **表格型方法**：
    表格型方法是一种直接使用表格来存储值函数或策略的方法。通常，这些表格的每一项对应于一个状态或状态-动作对，其值表示对应的状态值或状态-动作对值。
    在表格型方法中，智能体需要事先了解环境的状态空间和动作空间，并显式地维护一个状态值表格或状态-动作对值表格。智能体通过与环境交互来更新这些表格，从而学习值函数或策略。
    表格型方法包括动态规划、Q-learning 等。这些方法通常需要离散状态和动作空间，并且存储需占用大量内存。
总的来说，蒙特卡洛方法是一种基于经验的学习方法，通过与环境的交互来收集样本轨迹来估计值函数；而表格型方法则是一种直接使用表格来存储值函数或策略的方法，通常需要事先了解环境的状态空间和动作空间，并显式地维护一个状态值表格或状态-动作对值表格。
### 多智能体强化学习
	DQN（Deep Q-Network）、VDN（Value Decomposition Networks）和QMIX（Q-Value Interaction Networks）都是用于解决多智能体强化学习（MARL）问题的方法，它们都是基于Q值的算法。
##### 定义：DQN,IQL,COMA,VDN,QMIX
1. **DQN（Deep Q-Network）**：
    DQN是最早用于解决单智能体强化学习问题的深度强化学习算法之一。DQN使用深度神经网络来逼近Q值函数，即状态动作对的价值函数。DQN的目标是最小化预测的Q值与实际的Q值之间的均方误差，通过反向传播算法来更新神经网络的参数。
2. **IQL（Independent Q-Learning）:**
	独立学习，每个智能体都在环境中独立地学习。它们通过与环境的交互来收集数据，并使用 Q 学习等方法来更新其动作值函数。独立更新，在训练期间，每个智能体独立地更新其 Q 函数，而不考虑其他智能体的行为。这意味着它们通过与环境的交互来更新自己的策略，而不是考虑其他智能体如何行动。
	尽管 IQL 方法简单直观，但它也存在一些问题，特别是在存在多个智能体的情况下，由于每个智能体的学习过程是相互独立的，可能会导致不稳定性和子优化行为。因此，在某些情况下，更复杂的 MARL 方法可能会更有效
3. **COMA（Counterfactual Multi-Agent）：**
	COMA 算法专注于解决合作中的问题，如对策反应、收敛性和资源利用效率等。COMA 算法采用中心化价值函数来评估智能体的策略。这意味着智能体的价值函数不仅考虑了自身的奖励，还考虑了其他智能体的行为对整体回报的影响。这有助于智能体更好地协调其行为以达到整体最优化。
4. **VDN（Value Decomposition Networks）**：
    VDN是一种用于多智能体强化学习的算法，旨在解决多智能体共同决策问题。VDN将全局Q值函数分解为各个智能体的局部Q值函数之和，从而将多智能体的问题转化为单智能体的问题。VDN中的每个智能体都有自己的Q值函数，但这些Q值函数之间并不直接交互。
5. **QMIX（Q-Value Interaction Networks）**：
    QMIX是一种用于多智能体强化学习的算法，旨在学习智能体之间的复杂互动。QMIX引入了一个称为混合器（mixer）的神经网络，用于组合各个智能体的局部Q值函数，从而获得全局的Q值函数。QMIX的混合器允许局部Q值函数之间进行交互，这使得QMIX能够学习智能体之间的复杂协作和竞争关系。
##### 联系：
- DQN是解决单智能体强化学习问题的算法，而VDN和QMIX则是解决多智能体强化学习问题的算法。
- VDN和QMIX都使用了Q值函数的分解或组合方法，但QMIX引入了一个额外的混合器网络来处理智能体之间的互动，这使得QMIX能够处理更复杂的多智能体问题。
总的来说，DQN、VDN和QMIX都是用于解决不同类型的强化学习问题的方法，它们的设计思想和应用场景有所不同，但都是基于Q值的算法。